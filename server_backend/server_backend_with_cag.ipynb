{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ff4d6903e58d435a9f2b53c26bc53d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "Server is Starting Up... Elapsed Time:",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ecba2570b954f2ba0a7ccc7f7c8dd11",
            "placeholder": "​",
            "style": "IPY_MODEL_9567641fa46245808bd08dbff97c2ff0",
            "value": "18 Seconds"
          }
        },
        "5ecba2570b954f2ba0a7ccc7f7c8dd11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9567641fa46245808bd08dbff97c2ff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# If this complains about dependency resolver, it's safe to ignore\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noIkv-nXAkpe",
        "outputId": "e5b49c6b-dfde-4229-8f2f-34ccfd46f284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.2)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (0.0.20)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.4)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.11/dist-packages (4.13.0)\n",
            "Collecting bcrypt\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: fastapi[all] in /usr/local/lib/python3.11/dist-packages (0.115.12)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[all]) (0.46.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[all]) (4.13.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (0.0.7)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[all]) (0.28.1)\n",
            "Requirement already satisfied: jinja2>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from fastapi[all]) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[all]) (2.2.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from fastapi[all]) (6.0.2)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi[all])\n",
            "  Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: orjson>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from fastapi[all]) (3.10.18)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[all]) (2.2.0)\n",
            "Collecting pydantic-settings>=2.0.0 (from fastapi[all])\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting pydantic-extra-types>=2.0.0 (from fastapi[all])\n",
            "  Downloading pydantic_extra_types-2.10.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo) (2.7.0)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[all]) (3.10)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (0.15.3)\n",
            "Requirement already satisfied: rich-toolkit>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (0.14.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[all]) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[all]) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[all]) (1.0.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.1.5->fastapi[all]) (3.0.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings>=2.0.0->fastapi[all]) (1.1.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all]) (0.6.4)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all]) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all]) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all]) (15.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->fastapi[all]) (1.3.1)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (13.9.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (0.1.2)\n",
            "Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.4-py3-none-any.whl (37 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ujson, bcrypt, pydantic-settings, pydantic-extra-types\n",
            "Successfully installed bcrypt-4.3.0 pydantic-extra-types-2.10.4 pydantic-settings-2.9.1 ujson-5.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install\n",
        "!pip install fastapi[all] uvicorn python-multipart  pydantic  pymongo bcrypt pymongo pymupdf vllm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fTPfKaphw4u",
        "outputId": "ce7f88bf-dc10-4b5c-c54e-6aeddbcc3de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo\n",
            "  Downloading pymongo-4.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo) (2.7.0)\n",
            "Downloading pymongo-4.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymongo\n",
            "Successfully installed pymongo-4.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This downloads and sets up the Ngrok executable in the Google Colab instance\n",
        "# Import the ngrok GPG key\n",
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | gpg --import -\n",
        "\n",
        "# Add the ngrok repository to the apt sources list\n",
        "!echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list\n",
        "\n",
        "# Fetch the public key associated with the ngrok repository\n",
        "!sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 0E61D3BBAAEE37FE\n",
        "\n",
        "# Update the apt package lists\n",
        "!sudo apt-get update\n",
        "\n",
        "# Install ngrok\n",
        "!sudo apt-get install ngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH_wH8XWGspf",
        "outputId": "7d35afe3-cdcf-4b81-9204-ad6b9e07e10d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpg: directory '/root/.gnupg' created\n",
            "gpg: keybox '/root/.gnupg/pubring.kbx' created\n",
            "gpg: /root/.gnupg/trustdb.gpg: trustdb created\n",
            "gpg: key 0E61D3BBAAEE37FE: public key \"ngrok agent apt repo release bot <release-bot@ngrok.com>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "deb https://ngrok-agent.s3.amazonaws.com buster main\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "Executing: /tmp/apt-key-gpghome.Bu7JdZ1uNg/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 0E61D3BBAAEE37FE\n",
            "gpg: key 0E61D3BBAAEE37FE: public key \"ngrok agent apt repo release bot <release-bot@ngrok.com>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,676 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://ngrok-agent.s3.amazonaws.com buster InRelease [20.3 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:10 https://ngrok-agent.s3.amazonaws.com buster/main amd64 Packages [8,075 B]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,363 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,723 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,546 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,517 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,934 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,245 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Get:22 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,939 kB]\n",
            "Fetched 30.5 MB in 6s (5,405 kB/s)\n",
            "Reading package lists... Done\n",
            "W: https://ngrok-agent.s3.amazonaws.com/dists/buster/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  ngrok\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 6,777 kB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 https://ngrok-agent.s3.amazonaws.com buster/main amd64 ngrok amd64 3.22.1 [6,777 kB]\n",
            "Fetched 6,777 kB in 2s (2,869 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package ngrok.\n",
            "(Reading database ... 126102 files and directories currently installed.)\n",
            "Preparing to unpack .../ngrok_3.22.1_amd64.deb ...\n",
            "Unpacking ngrok (3.22.1) ...\n",
            "Setting up ngrok (3.22.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://dashboard.ngrok.com/signup\n",
        "!ngrok authtoken 2uZINKAUETNXWONds7xDf4VDfBT_7M7FgTzgKVxByGNy1EtBf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsIBRSPjRDA0",
        "outputId": "d7d617be-8ddc-4b09-accd-9c529ad0d68d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cag.py\n",
        "from typing import List, Tuple, Dict\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers.cache_utils import DynamicCache\n",
        "import re\n",
        "import uuid\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "import time\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Add DynamicCache to safe globals for serialization\n",
        "torch.serialization.add_safe_globals([DynamicCache])\n",
        "torch.serialization.add_safe_globals([set])\n",
        "\n",
        "\n",
        "class ContextualCAG:\n",
        "\n",
        "    def __init__(self, conversation_memory_size=5):\n",
        "        \"\"\"\n",
        "        Khởi tạo hệ thống CAG có tính ngữ cảnh cuộc hội thoại với KV cache\n",
        "\n",
        "        Args:\n",
        "            conversation_memory_size: Số lượng cuộc hội thoại gần nhất được lưu trữ\n",
        "        \"\"\"\n",
        "        logger.info(\"Initializing ContextualCAG\")\n",
        "\n",
        "        # Model name\n",
        "        self.llm_model_name = \"AITeamVN/Vi-Qwen2-1.5B-RAG\"\n",
        "\n",
        "        # Load LLM and tokenizer\n",
        "        logger.info(f\"Loading LLM model: {self.llm_model_name}\")\n",
        "        try:\n",
        "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.llm_model_name)\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.llm_model_name,\n",
        "                torch_dtype=torch.float16,\n",
        "                device_map=\"auto\"\n",
        "            )\n",
        "            logger.info(\"Model and tokenizer loaded successfully.\")\n",
        "            logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load LLM model: {e}\")\n",
        "            self.model = None\n",
        "            self.tokenizer = None\n",
        "\n",
        "        # Conversation tracking\n",
        "        self.conversation_history: List[Dict[str, str]] = []\n",
        "        self.conversation_memory_size = conversation_memory_size\n",
        "        self.current_session_id: str | None = None\n",
        "        self.sessions: Dict[str, List[Dict[str, str]]] = {}\n",
        "\n",
        "        # Store the knowledge cache\n",
        "        self.knowledge_cache: DynamicCache | None = None\n",
        "        self.cache_dir = \"./cache\"\n",
        "        os.makedirs(self.cache_dir, exist_ok=True)\n",
        "\n",
        "    def extract_clean_text_from_pdf(self, path: str) -> str:\n",
        "        \"\"\"\n",
        "        Đọc và xử lý tệp PDF thành văn bản sạch\n",
        "\n",
        "        Args:\n",
        "            path: Đường dẫn đến tệp PDF\n",
        "\n",
        "        Returns:\n",
        "            str: Văn bản đã được làm sạch từ PDF\n",
        "        \"\"\"\n",
        "        logger.info(f\"Extracting text from PDF: {path}\")\n",
        "\n",
        "        if not os.path.exists(path):\n",
        "            logger.error(f\"PDF file not found: {path}\")\n",
        "            return \"\"\n",
        "\n",
        "        try:\n",
        "            doc = fitz.open(path)\n",
        "            raw_text = \"\"\n",
        "\n",
        "            for page in doc:\n",
        "                raw_text += page.get_text()\n",
        "\n",
        "            clean_text = re.sub(r'\\s+', ' ', raw_text)\n",
        "            clean_text = clean_text.strip()\n",
        "\n",
        "            doc.close()\n",
        "            logger.info(f\"Extracted {len(clean_text)} characters from PDF.\")\n",
        "            return clean_text\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting text from PDF: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def start_new_session(self):\n",
        "        \"\"\"\n",
        "        Bắt đầu một phiên hội thoại mới\n",
        "\n",
        "        Returns:\n",
        "            str: ID phiên mới\n",
        "        \"\"\"\n",
        "        self.current_session_id = str(uuid.uuid4())\n",
        "        self.sessions[self.current_session_id] = []\n",
        "        self.conversation_history = []\n",
        "        logger.info(f\"Started new session: {self.current_session_id}\")\n",
        "        return self.current_session_id\n",
        "\n",
        "    def load_session(self, session_id):\n",
        "        \"\"\"\n",
        "        Tải một phiên hội thoại đã lưu\n",
        "\n",
        "        Args:\n",
        "            session_id: ID phiên cần tải\n",
        "\n",
        "        Returns:\n",
        "            bool: True nếu tải thành công, False nếu không\n",
        "        \"\"\"\n",
        "        if session_id in self.sessions:\n",
        "            self.current_session_id = session_id\n",
        "            self.conversation_history = self.sessions[session_id][-self.conversation_memory_size:]\n",
        "            logger.info(f\"Loaded session: {self.current_session_id}\")\n",
        "            return True\n",
        "        logger.warning(f\"Session ID not found: {session_id}\")\n",
        "        return False\n",
        "\n",
        "    def add_to_conversation(self, query, response):\n",
        "        \"\"\"\n",
        "        Thêm một cặp hỏi đáp vào lịch sử cuộc trò chuyện\n",
        "\n",
        "        Args:\n",
        "            query: Câu hỏi của người dùng\n",
        "            response: Câu trả lời của hệ thống\n",
        "        \"\"\"\n",
        "        if self.current_session_id is None:\n",
        "            self.start_new_session()\n",
        "\n",
        "        conv_pair = {\"query\": query, \"response\": response}\n",
        "\n",
        "        # Add to the full session history\n",
        "        self.sessions[self.current_session_id].append(conv_pair)\n",
        "\n",
        "        # Update the active conversation history (limited by memory size)\n",
        "        self.conversation_history = self.sessions[self.current_session_id][-self.conversation_memory_size:]\n",
        "        logger.debug(f\"Added to conversation history. Current length: {len(self.conversation_history)}\")\n",
        "\n",
        "    def clean_up(self, kv: DynamicCache, origin_len: int):\n",
        "        \"\"\"\n",
        "        Clean up KV cache to remove generated tokens\n",
        "\n",
        "        Args:\n",
        "            kv: KV cache to clean up\n",
        "            origin_len: Original length to keep\n",
        "        \"\"\"\n",
        "        for i in range(len(kv.key_cache)):\n",
        "            kv.key_cache[i] = kv.key_cache[i][:, :, :origin_len, :]\n",
        "            kv.value_cache[i] = kv.value_cache[i][:, :, :origin_len, :]\n",
        "\n",
        "    def generate(self, input_ids: torch.Tensor, past_key_values: DynamicCache, max_new_tokens: int = 150) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Generate response using the model with KV cache\n",
        "\n",
        "        Args:\n",
        "            input_ids: Input token IDs\n",
        "            past_key_values: KV cache from knowledge preprocessing\n",
        "            max_new_tokens: Maximum number of new tokens to generate\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Generated token IDs\n",
        "        \"\"\"\n",
        "        embed_device = self.model.model.embed_tokens.weight.device\n",
        "        origin_ids = input_ids\n",
        "        input_ids = input_ids.to(embed_device)\n",
        "\n",
        "        output_ids = input_ids.clone()\n",
        "        next_token = input_ids\n",
        "\n",
        "        im_end_id = self.tokenizer.encode(\"<|im_end|>\", add_special_tokens=False)[0]\n",
        "        newline_id = self.tokenizer.encode(\"\\n\", add_special_tokens=False)[0]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_new_tokens):\n",
        "                outputs = self.model(\n",
        "                    input_ids=next_token,\n",
        "                    past_key_values=past_key_values,\n",
        "                    use_cache=True\n",
        "                )\n",
        "                next_token_logits = outputs.logits[:, -1, :]\n",
        "                next_token = next_token_logits.argmax(dim=-1).unsqueeze(-1).to(embed_device)\n",
        "\n",
        "                past_key_values = outputs.past_key_values\n",
        "                output_ids = torch.cat([output_ids, next_token], dim=1)\n",
        "\n",
        "                if next_token.item() == im_end_id:\n",
        "                    break\n",
        "                if next_token.item() == newline_id and output_ids.shape[1] > origin_ids.shape[1] + 5:\n",
        "                    break\n",
        "\n",
        "                eos_token_id = self.model.config.eos_token_id\n",
        "                if isinstance(eos_token_id, int):\n",
        "                    eos_token_id = [eos_token_id]\n",
        "                if next_token.item() in eos_token_id:\n",
        "                    break\n",
        "\n",
        "                if output_ids.shape[1] > origin_ids.shape[1] + 10:\n",
        "                    text_so_far = self.tokenizer.decode(output_ids[0, origin_ids.shape[1]:], skip_special_tokens=True)\n",
        "                    if text_so_far.strip().endswith('.'):\n",
        "                        break\n",
        "\n",
        "        return output_ids[:, origin_ids.shape[-1]:]\n",
        "\n",
        "    def preprocess_knowledge(self, prompt: str) -> DynamicCache:\n",
        "        \"\"\"\n",
        "        Preprocess knowledge into KV cache for efficient reuse\n",
        "\n",
        "        Args:\n",
        "            prompt: Knowledge prompt to preprocess\n",
        "\n",
        "        Returns:\n",
        "            DynamicCache: Preprocessed KV cache\n",
        "        \"\"\"\n",
        "        embed_device = self.model.model.embed_tokens.weight.device\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False).to(embed_device)\n",
        "        past_key_values = DynamicCache()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(\n",
        "                input_ids=input_ids,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=True,\n",
        "                output_attentions=False,\n",
        "                output_hidden_states=False\n",
        "            )\n",
        "        return outputs.past_key_values\n",
        "\n",
        "    def write_kv_cache(self, kv: DynamicCache, path: str):\n",
        "        \"\"\"Save KV cache to file\"\"\"\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        torch.save(kv, path)\n",
        "        logger.info(f\"KV cache saved to {path}\")\n",
        "\n",
        "    def read_kv_cache(self, path: str) -> DynamicCache | None:\n",
        "        \"\"\"Load KV cache from file\"\"\"\n",
        "        if os.path.exists(path) and os.path.getsize(path) > 0:\n",
        "            kv = torch.load(path, weights_only=True)\n",
        "            logger.info(f\"KV cache loaded from {path}\")\n",
        "            return kv\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def process_pdf_document(self, pdf_path: str, cache_file: str = None):\n",
        "        \"\"\"\n",
        "        Process PDF document and create KV cache\n",
        "\n",
        "        Args:\n",
        "            pdf_path: Path to PDF file\n",
        "            cache_file: Optional path to save/load cache\n",
        "        \"\"\"\n",
        "        if self.model is None or self.tokenizer is None:\n",
        "            logger.error(\"Model or Tokenizer not loaded. Cannot process document.\")\n",
        "            return False\n",
        "\n",
        "        logger.info(f\"Processing document: {pdf_path}\")\n",
        "\n",
        "        # Extract text from PDF\n",
        "        knowledge_text = self.extract_clean_text_from_pdf(pdf_path)\n",
        "\n",
        "        if not knowledge_text:\n",
        "            logger.warning(\"No text extracted from PDF.\")\n",
        "            return False\n",
        "\n",
        "        # Check if we can load existing cache\n",
        "        if cache_file is None:\n",
        "            cache_file = os.path.join(self.cache_dir, \"knowledge_cache.pt\")\n",
        "\n",
        "        existing_cache = self.read_kv_cache(cache_file)\n",
        "        if existing_cache is not None:\n",
        "            self.knowledge_cache = existing_cache\n",
        "            logger.info(\"Using existing KV cache\")\n",
        "            return True\n",
        "\n",
        "        # Create knowledge prompt\n",
        "        knowledge_prompt = f\"\"\"<|im_start|>system\n",
        "Bạn là một hướng dẫn viên du lịch. Hãy trả lời các câu hỏi dựa trên ngữ cảnh được cung cấp.\n",
        "Quy tắc trả lời:\n",
        "1. Trả lời bằng tiếng Việt\n",
        "2. Diễn giải câu trả lời thành một đoạn văn tự nhiên\n",
        "3. Chỉ sử dụng thông tin từ ngữ cảnh được cung cấp\n",
        "4. Không thêm thông tin ngoài ngữ cảnh\n",
        "5. Sử dụng ngôn ngữ tự nhiên, dễ hiểu<|im_end|>\n",
        "<|im_start|>user\n",
        "Ngữ cảnh:\n",
        "{knowledge_text}\n",
        "\n",
        "Trả lời câu hỏi sau dựa trên ngữ cảnh được cung cấp. Trả lời bằng **TIẾNG VIỆT**, ngắn gọn nhưng đầy đủ thông tin.\n",
        "\"\"\"\n",
        "\n",
        "        # Preprocess knowledge into KV cache\n",
        "        logger.info(\"Creating KV cache from knowledge...\")\n",
        "        t1 = time.time()\n",
        "        self.knowledge_cache = self.preprocess_knowledge(knowledge_prompt)\n",
        "        t2 = time.time()\n",
        "        logger.info(f\"KV cache created in {t2 - t1:.2f} seconds\")\n",
        "\n",
        "        # Save cache to file\n",
        "        self.write_kv_cache(self.knowledge_cache, cache_file)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def generate_conversation_context(self) -> str:\n",
        "        \"\"\"\n",
        "        Tạo ngữ cảnh từ lịch sử cuộc trò chuyện\n",
        "\n",
        "        Returns:\n",
        "            str: Chuỗi text lịch sử trò chuyện hoặc chuỗi rỗng\n",
        "        \"\"\"\n",
        "        if not self.conversation_history:\n",
        "            return \"\"\n",
        "\n",
        "        context = \"Lịch sử trò chuyện gần đây:\\n\"\n",
        "        for conv in self.conversation_history:\n",
        "            context += f\"Người dùng: {conv['query']}\\n\"\n",
        "            context += f\"Trợ lý: {conv['response']}\\n\"\n",
        "\n",
        "        return context\n",
        "\n",
        "    def contextual_qa_chat(self, query: str, max_new_tokens: int = 150):\n",
        "        \"\"\"\n",
        "        Trả lời câu hỏi của người dùng, kết hợp kiến thức từ tài liệu và lịch sử hội thoại\n",
        "\n",
        "        Args:\n",
        "            query: Câu hỏi hiện tại của người dùng\n",
        "            max_new_tokens: Số lượng token mới tối đa cho câu trả lời\n",
        "\n",
        "        Returns:\n",
        "            str: Câu trả lời của AI\n",
        "        \"\"\"\n",
        "        if self.model is None or self.tokenizer is None:\n",
        "            return \"Hệ thống chưa sẵn sàng. Vui lòng kiểm tra quá trình tải mô hình.\"\n",
        "\n",
        "        if self.knowledge_cache is None:\n",
        "            logger.warning(\"No knowledge cache available. Please process a document first.\")\n",
        "            return \"Vui lòng tải tài liệu trước khi đặt câu hỏi.\"\n",
        "\n",
        "        # Create query prompt with conversation history\n",
        "        conversation_context = self.generate_conversation_context()\n",
        "\n",
        "        prompt = \"\"\n",
        "        if conversation_context:\n",
        "            prompt += conversation_context + \"\\n\\n\"\n",
        "\n",
        "        prompt += f\"\"\"Câu hỏi: {query}\n",
        "\n",
        "Yêu cầu: Trả lời bằng tiếng Việt, diễn giải thành đoạn văn tự nhiên dựa trên ngữ cảnh đã cung cấp.<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "        # Create a copy of the knowledge cache\n",
        "        cache_copy = DynamicCache()\n",
        "        for j in range(len(self.knowledge_cache.key_cache)):\n",
        "            cache_copy.key_cache.append(self.knowledge_cache.key_cache[j].clone())\n",
        "            cache_copy.value_cache.append(self.knowledge_cache.value_cache[j].clone())\n",
        "\n",
        "        # Generate response\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False).to(self.model.device)\n",
        "\n",
        "        t1 = time.time()\n",
        "        output = self.generate(input_ids, cache_copy, max_new_tokens)\n",
        "        t2 = time.time()\n",
        "\n",
        "        # Decode and clean the generated text\n",
        "        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
        "\n",
        "        # Clean up any remaining special tokens\n",
        "        for tag in [\"<|im_end|>\", \"<|im_start|>\", \"assistant\", \"user\", \"system\"]:\n",
        "            generated_text = generated_text.replace(tag, \"\")\n",
        "        generated_text = generated_text.strip()\n",
        "\n",
        "        # Optional: Clean up the cache to original length\n",
        "        # origin_len = len(self.tokenizer.encode(self.knowledge_prompt, add_special_tokens=False))\n",
        "        # self.clean_up(cache_copy, origin_len)\n",
        "\n",
        "        logger.info(f\"Generated response in {t2 - t1:.2f} seconds\")\n",
        "\n",
        "        # Add to conversation history\n",
        "        self.add_to_conversation(query, generated_text)\n",
        "\n",
        "        return generated_text\n",
        "\n",
        "    def save_conversation_history(self, file_path):\n",
        "        \"\"\"\n",
        "        Lưu toàn bộ lịch sử các phiên cuộc trò chuyện\n",
        "        \"\"\"\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "        history_data = {}\n",
        "        for session_id, conversations in self.sessions.items():\n",
        "            history_data[session_id] = [\n",
        "                {\"query\": c[\"query\"], \"response\": c[\"response\"]}\n",
        "                for c in conversations\n",
        "            ]\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(history_data, f, ensure_ascii=False, indent=2)\n",
        "            logger.info(f\"Conversation history saved to {file_path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving conversation history to {file_path}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_conversation_history(self, file_path):\n",
        "        \"\"\"\n",
        "        Tải toàn bộ lịch sử các phiên cuộc trò chuyện\n",
        "        \"\"\"\n",
        "        if not os.path.exists(file_path):\n",
        "            logger.info(f\"Conversation history file not found: {file_path}\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                history_data = json.load(f)\n",
        "\n",
        "            self.sessions = history_data\n",
        "            logger.info(f\"Loaded conversation history containing {len(self.sessions)} sessions.\")\n",
        "\n",
        "            # After loading, if there's a last session, set it as the current one\n",
        "            if self.sessions:\n",
        "                last_session_id = list(self.sessions.keys())[-1]\n",
        "                self.current_session_id = last_session_id\n",
        "                self.conversation_history = self.sessions[self.current_session_id][-self.conversation_memory_size:]\n",
        "                logger.info(f\"Set current session to: {self.current_session_id}\")\n",
        "            else:\n",
        "                self.current_session_id = None\n",
        "                self.conversation_history = []\n",
        "\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading conversation history from {file_path}: {e}\")\n",
        "            self.sessions = {}\n",
        "            self.current_session_id = None\n",
        "            self.conversation_history = []\n",
        "            return False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9HwaFUlHoba",
        "outputId": "5195d6a2-5d53-4f8e-bf36-858a12c9e45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cag.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_cag.py\n",
        "from typing import Any, List, Optional\n",
        "# Thêm các import cần thiết\n",
        "from fastapi import FastAPI, HTTPException, UploadFile, File, Form, Request, Response, Depends\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel, Field\n",
        "import torch\n",
        "import os\n",
        "import uuid\n",
        "from contextlib import asynccontextmanager\n",
        "from pymongo import MongoClient\n",
        "from pymongo.errors import ConnectionFailure, OperationFailure\n",
        "import bcrypt\n",
        "# Import class CAG thay vì RAG\n",
        "from cag import ContextualCAG # Đảm bảo file cag.py cùng thư mục hoặc trong PYTHONPATH\n",
        "from bson.objectid import ObjectId\n",
        "import json\n",
        "import logging\n",
        "import time # Thêm time nếu cần đo thời gian xử lý\n",
        "\n",
        "# Cấu hình logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Khởi tạo lifespan của ứng dụng để tải mô hình CAG\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    logger.info(\"Starting application lifespan for CAG...\")\n",
        "    # Initialize CAG model\n",
        "    try:\n",
        "        # Thay ContextualRetrieval bằng ContextualCAG\n",
        "        app.state.cag_model = ContextualCAG()\n",
        "        logger.info(\"CAG model initialized.\")\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Failed to initialize CAG model during startup!\")\n",
        "        raise RuntimeError(\"CAG model initialization failed\") from e\n",
        "\n",
        "    # Load existing conversation history if available\n",
        "    history_path = \"conversation_history.json\"\n",
        "    if os.path.exists(history_path):\n",
        "        try:\n",
        "            # Gọi phương thức của cag_model\n",
        "            loaded = app.state.cag_model.load_conversation_history(history_path)\n",
        "            if loaded:\n",
        "                logger.info(f\"Loaded conversation history from {history_path}\")\n",
        "            else:\n",
        "                logger.warning(f\"Failed to load conversation history from {history_path}, but file exists.\")\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"Error loading conversation history from {history_path}\")\n",
        "    else:\n",
        "        logger.info(f\"Conversation history file not found at {history_path}, starting fresh.\")\n",
        "\n",
        "    # Ensure upload directory exists\n",
        "    upload_dir = \"uploaded_files\"\n",
        "    if not os.path.exists(upload_dir):\n",
        "        try:\n",
        "            os.makedirs(upload_dir)\n",
        "            logger.info(f\"Created directory: {upload_dir}\")\n",
        "        except OSError as e:\n",
        "            logger.error(f\"Failed to create upload directory {upload_dir}: {e}\")\n",
        "\n",
        "    # --- Kết nối MongoDB ---\n",
        "    MONGO_URI = os.getenv(\"MONGO_URI\", \"mongodb+srv://hoangkahn123:0968269517@cluster0.8to29z8.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\")\n",
        "    app.state.mongo_client = None\n",
        "    app.state.users_collection = None\n",
        "    try:\n",
        "        logger.info(f\"Connecting to MongoDB at {MONGO_URI.split('@')[-1].split('/')[0]}...\")\n",
        "        app.state.mongo_client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)\n",
        "        app.state.mongo_client.admin.command('ping')\n",
        "        db = app.state.mongo_client[\"userdb\"]\n",
        "        app.state.users_collection = db[\"users\"]\n",
        "        logger.info(\"Successfully connected to MongoDB.\")\n",
        "    except ConnectionFailure as e:\n",
        "        logger.error(f\"Failed to connect to MongoDB: {e}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An unexpected error occurred during MongoDB connection: {e}\")\n",
        "    # -----------------------\n",
        "\n",
        "    yield # Application is ready to serve requests\n",
        "\n",
        "    # --- Shutdown ---\n",
        "    logger.info(\"Shutting down application lifespan (CAG)...\")\n",
        "    # Save conversation history before shutdown\n",
        "    if hasattr(app.state, 'cag_model'):\n",
        "        try:\n",
        "            app.state.cag_model.save_conversation_history(\"conversation_history.json\")\n",
        "            logger.info(\"Saved conversation history.\")\n",
        "        except Exception as e:\n",
        "            logger.exception(\"Error saving conversation history during shutdown\")\n",
        "\n",
        "    # Close MongoDB connection\n",
        "    if app.state.mongo_client:\n",
        "        try:\n",
        "            app.state.mongo_client.close()\n",
        "            logger.info(\"MongoDB connection closed.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error closing MongoDB connection: {e}\")\n",
        "    # ----------------\n",
        "\n",
        "app = FastAPI(lifespan=lifespan)\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"http://localhost:5173\"], # Cập nhật cho production\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\", \"Authorization\"],\n",
        ")\n",
        "\n",
        "# Các model Pydantic\n",
        "class UserCredentials(BaseModel):\n",
        "    username: str\n",
        "    password: str\n",
        "\n",
        "class LoginResponse(BaseModel):\n",
        "    username: str\n",
        "    id: str\n",
        "    session_id: str\n",
        "\n",
        "class RegisterResponse(BaseModel):\n",
        "    message: str\n",
        "    session_id: str\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    query: str\n",
        "    max_new_tokens: Optional[int] = 150  # Thêm option cho max_new_tokens\n",
        "\n",
        "class ChatResponse(BaseModel):\n",
        "    response: str\n",
        "    session_id: str\n",
        "\n",
        "class SessionResponse(BaseModel):\n",
        "    session_id: str\n",
        "    message: str\n",
        "\n",
        "class DocumentUploadResponse(BaseModel):\n",
        "    message: str\n",
        "    document_id: str\n",
        "\n",
        "class StatusResponse(BaseModel):\n",
        "    status: str\n",
        "    gpu: str\n",
        "\n",
        "class SessionListResponse(BaseModel):\n",
        "    sessions: List[str]\n",
        "\n",
        "class HistoryItem(BaseModel):\n",
        "    query: Optional[str] = None\n",
        "    response: Optional[str] = None\n",
        "    timestamp: Optional[str] = None\n",
        "\n",
        "class HistoryResponse(BaseModel):\n",
        "    session_id: str\n",
        "    history: List[HistoryItem]\n",
        "\n",
        "class LogoutResponse(BaseModel):\n",
        "    message: str\n",
        "\n",
        "# Hàm phụ thuộc để lấy DB collection\n",
        "def get_users_collection():\n",
        "    if not hasattr(app.state, 'users_collection') or app.state.users_collection is None:\n",
        "        logger.error(\"Users collection is not available.\")\n",
        "        raise HTTPException(status_code=503, detail=\"Database service unavailable\")\n",
        "    return app.state.users_collection\n",
        "\n",
        "# Hàm phụ thuộc để lấy CAG model\n",
        "def get_cag_model() -> ContextualCAG:\n",
        "    if not hasattr(app.state, 'cag_model') or app.state.cag_model is None:\n",
        "        logger.error(\"CAG model is not available.\")\n",
        "        raise HTTPException(status_code=503, detail=\"CAG service unavailable\")\n",
        "    return app.state.cag_model\n",
        "\n",
        "# Hàm phụ thuộc để lấy session từ token\n",
        "async def get_session_from_token(request: Request):\n",
        "    \"\"\"Extract and validate session token from Authorization header.\"\"\"\n",
        "    auth_header = request.headers.get(\"Authorization\")\n",
        "    if not auth_header or not auth_header.startswith(\"Bearer \"):\n",
        "        logger.warning(\"Access attempt failed: No valid Authorization header found.\")\n",
        "        raise HTTPException(status_code=401, detail=\"Unauthorized: Invalid or missing token\")\n",
        "\n",
        "    session_id = auth_header.split(\" \")[1]\n",
        "\n",
        "    # Sử dụng cag_model để kiểm tra session\n",
        "    cag_model = get_cag_model()\n",
        "    if not cag_model.load_session(session_id):\n",
        "        logger.warning(f\"Access attempt failed: Session ID '{session_id}' not found in CAG model memory.\")\n",
        "        raise HTTPException(status_code=401, detail=\"Invalid or expired session. Please login again.\")\n",
        "\n",
        "    return session_id\n",
        "\n",
        "# --- Endpoints API ---\n",
        "\n",
        "@app.get(\"/\", response_model=StatusResponse)\n",
        "def status_gpu_check():\n",
        "    \"\"\"Kiểm tra trạng thái API (CAG) và GPU.\"\"\"\n",
        "    cag_model = get_cag_model() # Check if CAG model loaded\n",
        "    gpu_available = torch.cuda.is_available()\n",
        "    gpu_msg = f\"Available ({torch.cuda.get_device_name(0)})\" if gpu_available else \"Unavailable\"\n",
        "    return {\n",
        "        \"status\": \"CAG API is running\",\n",
        "        \"gpu\": gpu_msg\n",
        "    }\n",
        "\n",
        "@app.post(\"/upload/pdf\", response_model=DocumentUploadResponse)\n",
        "async def upload_pdf(file: UploadFile = File(...)):\n",
        "    \"\"\"Upload và xử lý file PDF để tạo KV cache.\"\"\"\n",
        "    cag_model = get_cag_model()\n",
        "\n",
        "    if not file.filename.endswith('.pdf'):\n",
        "        logger.warning(f\"Upload rejected: Non-PDF file '{file.filename}'\")\n",
        "        raise HTTPException(status_code=400, detail=\"Only PDF files are accepted\")\n",
        "\n",
        "    document_id = str(uuid.uuid4())\n",
        "    file_path = f\"uploaded_files/{document_id}.pdf\"\n",
        "    logger.info(f\"Received PDF upload: '{file.filename}', saving as {document_id}.pdf\")\n",
        "\n",
        "    try:\n",
        "        # Lưu file\n",
        "        file_content = await file.read()\n",
        "        if not file_content:\n",
        "            raise HTTPException(status_code=400, detail=\"Uploaded file is empty\")\n",
        "\n",
        "        with open(file_path, \"wb\") as buffer:\n",
        "            buffer.write(file_content)\n",
        "        logger.info(f\"Saved uploaded file to: {file_path}\")\n",
        "\n",
        "        # Xử lý tài liệu bằng CAG model\n",
        "        logger.info(f\"Processing PDF document with CAG: {document_id}.pdf\")\n",
        "\n",
        "        # Tạo cache file path cho document này\n",
        "        cache_file_path = os.path.join(app.state.cag_model.cache_dir, f\"{document_id}_cache.pt\")\n",
        "\n",
        "        # Gọi đúng phương thức với tham số đúng\n",
        "        processed_ok = cag_model.process_pdf_document(file_path, cache_file=cache_file_path)\n",
        "\n",
        "        if not processed_ok:\n",
        "            logger.error(f\"CAG model failed to process document {document_id}.pdf\")\n",
        "            raise HTTPException(status_code=500, detail=\"Failed to process PDF content.\")\n",
        "\n",
        "        logger.info(f\"PDF processed successfully by CAG. KV cache created for document {document_id}.\")\n",
        "\n",
        "        return {\n",
        "            \"message\": \"PDF uploaded and processed successfully by CAG\",\n",
        "            \"document_id\": document_id\n",
        "        }\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Unexpected error processing PDF {document_id}.pdf with CAG\")\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                os.remove(file_path)\n",
        "                logger.info(f\"Removed temporary file due to error: {file_path}\")\n",
        "            except OSError as rm_err:\n",
        "                logger.error(f\"Error removing file {file_path} after error: {rm_err}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Error processing PDF with CAG: {str(e)}\")\n",
        "    finally:\n",
        "        await file.close()\n",
        "\n",
        "@app.post(\"/chat\", response_model=ChatResponse)\n",
        "async def chat(request_body: ChatRequest, session_id: str = Depends(get_session_from_token)):\n",
        "    \"\"\"Xử lý yêu cầu chat, sử dụng session và KV cache từ CAG model.\"\"\"\n",
        "    cag_model = get_cag_model()\n",
        "\n",
        "    logger.info(f\"Received chat request for session: {session_id} | Query: '{request_body.query[:50]}...'\")\n",
        "\n",
        "    # Kiểm tra xem CAG model đã có knowledge cache chưa\n",
        "    if cag_model.knowledge_cache is None:\n",
        "        logger.warning(f\"Chat attempt failed for session {session_id}: No knowledge cache loaded in CAG model.\")\n",
        "        raise HTTPException(status_code=400, detail=\"No document has been uploaded and processed yet. Please upload a PDF first.\")\n",
        "\n",
        "    try:\n",
        "        # Xử lý chat request bằng CAG model\n",
        "        logger.info(f\"Processing query with CAG for session {session_id}...\")\n",
        "\n",
        "        # Sử dụng max_new_tokens từ request nếu có\n",
        "        max_new_tokens = request_body.max_new_tokens\n",
        "        response_text = cag_model.contextual_qa_chat(request_body.query, max_new_tokens=max_new_tokens)\n",
        "\n",
        "        logger.info(f\"Generated CAG response for session {session_id}\")\n",
        "\n",
        "        return {\n",
        "            \"response\": response_text,\n",
        "            \"session_id\": session_id\n",
        "        }\n",
        "    except HTTPException as http_exc:\n",
        "        raise http_exc\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Unexpected error during CAG chat processing for session {session_id}\")\n",
        "        raise HTTPException(status_code=500, detail=\"Internal server error during CAG chat processing\")\n",
        "\n",
        "@app.post(\"/chat/noauth\", response_model=ChatResponse)\n",
        "async def chat_no_auth(request_body: ChatRequest):\n",
        "    \"\"\"\n",
        "    Xử lý yêu cầu chat mà không yêu cầu xác thực người dùng.\n",
        "    \"\"\"\n",
        "    cag_model = get_cag_model()\n",
        "\n",
        "    # Kiểm tra xem đã có session_id cho anonymous user chưa\n",
        "    if not hasattr(app.state, 'anonymous_session_id') or app.state.anonymous_session_id is None:\n",
        "        app.state.anonymous_session_id = cag_model.start_new_session()\n",
        "        logger.info(f\"Created new anonymous session: {app.state.anonymous_session_id}\")\n",
        "    else:\n",
        "        # Load lại session nếu nó tồn tại\n",
        "        if cag_model.load_session(app.state.anonymous_session_id):\n",
        "            logger.info(f\"Resuming anonymous session: {app.state.anonymous_session_id}\")\n",
        "        else:\n",
        "            app.state.anonymous_session_id = cag_model.start_new_session()\n",
        "            logger.warning(f\"Anonymous session ID was present but invalid, created new: {app.state.anonymous_session_id}\")\n",
        "\n",
        "    session_id = app.state.anonymous_session_id\n",
        "\n",
        "    logger.info(f\"Received chat request (no auth) for session: {session_id} | Query: '{request_body.query[:50]}...'\")\n",
        "\n",
        "    # Kiểm tra xem CAG model đã có knowledge cache chưa\n",
        "    if cag_model.knowledge_cache is None:\n",
        "        logger.warning(f\"Chat attempt (no auth) failed for session {session_id}: No knowledge cache loaded in CAG model.\")\n",
        "        raise HTTPException(status_code=400, detail=\"No document has been uploaded and processed yet. Please upload a PDF first.\")\n",
        "\n",
        "    try:\n",
        "        # Xử lý chat request bằng CAG model\n",
        "        logger.info(f\"Processing query with CAG for session {session_id} (no auth)...\")\n",
        "\n",
        "        max_new_tokens = request_body.max_new_tokens\n",
        "        response_text = cag_model.contextual_qa_chat(request_body.query, max_new_tokens=max_new_tokens)\n",
        "\n",
        "        logger.info(f\"Generated CAG response (no auth) for session {session_id}\")\n",
        "\n",
        "        return {\n",
        "            \"response\": response_text,\n",
        "            \"session_id\": session_id\n",
        "        }\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Unexpected error during CAG chat processing (no auth) for session {session_id}\")\n",
        "        raise HTTPException(status_code=500, detail=\"Internal server error during CAG chat processing (no auth)\")\n",
        "\n",
        "# Các endpoint quản lý session và history\n",
        "@app.get(\"/sessions\", response_model=SessionListResponse)\n",
        "def list_sessions(session_id: str = Depends(get_session_from_token)):\n",
        "    \"\"\"Liệt kê tất cả các session IDs đang có trong CAG model memory.\"\"\"\n",
        "    cag_model = get_cag_model()\n",
        "    sessions = list(cag_model.sessions.keys())\n",
        "    return {\"sessions\": sessions}\n",
        "\n",
        "@app.get(\"/history\", response_model=HistoryResponse)\n",
        "def get_current_session_history(session_id: str = Depends(get_session_from_token)):\n",
        "    \"\"\"Lấy lịch sử trò chuyện của session hiện tại từ CAG model memory.\"\"\"\n",
        "    cag_model = get_cag_model()\n",
        "\n",
        "    history = []\n",
        "    if session_id in cag_model.sessions:\n",
        "        for conv in cag_model.sessions[session_id]:\n",
        "            history.append(HistoryItem(\n",
        "                query=conv.get(\"query\"),\n",
        "                response=conv.get(\"response\"),\n",
        "                timestamp=None\n",
        "            ))\n",
        "    else:\n",
        "        logger.warning(f\"Session {session_id} validated but not found in cag_model.sessions for history request.\")\n",
        "\n",
        "    return {\"session_id\": session_id, \"history\": history}\n",
        "\n",
        "@app.get(\"/history/{target_session_id}\", response_model=HistoryResponse)\n",
        "def get_specific_session_history(\n",
        "    target_session_id: str,\n",
        "    current_session_id: str = Depends(get_session_from_token)\n",
        "):\n",
        "    \"\"\"Lấy lịch sử trò chuyện của một session cụ thể từ CAG model memory.\"\"\"\n",
        "    cag_model = get_cag_model()\n",
        "    if target_session_id not in cag_model.sessions:\n",
        "        raise HTTPException(status_code=404, detail=\"Target session not found in CAG model memory\")\n",
        "\n",
        "    history = []\n",
        "    for conv in cag_model.sessions[target_session_id]:\n",
        "        history.append(HistoryItem(\n",
        "            query=conv.get(\"query\"),\n",
        "            response=conv.get(\"response\"),\n",
        "            timestamp=None\n",
        "        ))\n",
        "\n",
        "    return {\"session_id\": target_session_id, \"history\": history}\n",
        "\n",
        "@app.delete(\"/sessions\", response_model=SessionResponse)\n",
        "def delete_all_sessions(session_id: str = Depends(get_session_from_token)):\n",
        "    \"\"\"Xóa tất cả các session khỏi CAG model memory.\"\"\"\n",
        "    cag_model = get_cag_model()\n",
        "    count = len(cag_model.sessions)\n",
        "    cag_model.sessions.clear()\n",
        "    cag_model.current_session_id = None\n",
        "    cag_model.conversation_history = []\n",
        "    logger.warning(f\"Cleared all {count} sessions from CAG model memory.\")\n",
        "    return {\n",
        "        \"session_id\": \"all\",\n",
        "        \"message\": f\"All {count} sessions deleted successfully from memory\"\n",
        "    }\n",
        "\n",
        "# --- Endpoints Đăng ký / Đăng nhập / Đăng xuất ---\n",
        "\n",
        "@app.post(\"/register/\", response_model=RegisterResponse, status_code=201)\n",
        "async def register_user(user: UserCredentials):\n",
        "    \"\"\"Đăng ký người dùng mới.\"\"\"\n",
        "    users_collection = get_users_collection()\n",
        "    cag_model = get_cag_model()\n",
        "\n",
        "    existing_user = users_collection.find_one({\"username\": {\"$regex\": f\"^{user.username}$\", \"$options\": \"i\"}})\n",
        "    if existing_user:\n",
        "        logger.warning(f\"Registration failed: Username '{user.username}' already exists.\")\n",
        "        raise HTTPException(status_code=400, detail=\"Username already exists\")\n",
        "\n",
        "    try:\n",
        "        hashed_password = bcrypt.hashpw(user.password.encode(\"utf-8\"), bcrypt.gensalt())\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error hashing password for {user.username}: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=\"Error processing password\")\n",
        "\n",
        "    # Tạo session CAG mới cho user\n",
        "    session_id = cag_model.start_new_session()\n",
        "    logger.info(f\"Started new CAG session {session_id} for new user {user.username}\")\n",
        "\n",
        "    try:\n",
        "        user_data = {\n",
        "            \"username\": user.username,\n",
        "            \"password\": hashed_password,\n",
        "            \"session_id\": session_id\n",
        "        }\n",
        "        insert_result = users_collection.insert_one(user_data)\n",
        "        logger.info(f\"Registered user {user.username} with DB ID {insert_result.inserted_id} and CAG session {session_id}\")\n",
        "        return RegisterResponse(message=\"User registered successfully\", session_id=session_id)\n",
        "    except OperationFailure as e:\n",
        "        logger.error(f\"Database error during registration for {user.username}: {e.details}\")\n",
        "        # Rollback: Xóa session vừa tạo\n",
        "        if session_id in cag_model.sessions:\n",
        "            del cag_model.sessions[session_id]\n",
        "            if cag_model.current_session_id == session_id:\n",
        "                cag_model.current_session_id = None\n",
        "                cag_model.conversation_history = []\n",
        "            logger.info(f\"Rolled back CAG session {session_id} due to DB error.\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Database error during registration\")\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Unexpected error registering user {user.username}\")\n",
        "        raise HTTPException(status_code=500, detail=\"Internal server error during registration\")\n",
        "\n",
        "@app.post(\"/login/\", response_model=LoginResponse)\n",
        "async def login_user(user: UserCredentials):\n",
        "    \"\"\"Đăng nhập người dùng và trả về token (session_id của CAG).\"\"\"\n",
        "    users_collection = get_users_collection()\n",
        "    cag_model = get_cag_model()\n",
        "    logger.info(f\"Login attempt for user: {user.username}\")\n",
        "\n",
        "    found_user = users_collection.find_one({\"username\": {\"$regex\": f\"^{user.username}$\", \"$options\": \"i\"}})\n",
        "\n",
        "    if found_user and bcrypt.checkpw(user.password.encode(\"utf-8\"), found_user[\"password\"]):\n",
        "        user_id_str = str(found_user[\"_id\"])\n",
        "        session_id = found_user.get(\"session_id\")\n",
        "        session_loaded_or_created = False\n",
        "\n",
        "        if session_id and cag_model.load_session(session_id):\n",
        "            logger.info(f\"Loaded existing CAG session {session_id} for user {user.username}\")\n",
        "            session_loaded_or_created = True\n",
        "        else:\n",
        "            if session_id:\n",
        "                logger.warning(f\"Session ID {session_id} for user {user.username} found in DB but not in CAG model memory. Starting new session.\")\n",
        "            else:\n",
        "                logger.warning(f\"No session ID found in DB for user {user.username}. Starting new session.\")\n",
        "\n",
        "            # Tạo session CAG mới và cập nhật DB\n",
        "            session_id = cag_model.start_new_session()\n",
        "            try:\n",
        "                update_result = users_collection.update_one(\n",
        "                    {\"_id\": found_user[\"_id\"]},\n",
        "                    {\"$set\": {\"session_id\": session_id}}\n",
        "                )\n",
        "                if update_result.modified_count == 1:\n",
        "                    logger.info(f\"Updated user {user.username} in DB with new CAG session ID {session_id}\")\n",
        "                    session_loaded_or_created = True\n",
        "                else:\n",
        "                    logger.error(f\"Failed to update session ID in DB for user {user.username}\")\n",
        "                    if session_id in cag_model.sessions: del cag_model.sessions[session_id]\n",
        "                    raise HTTPException(status_code=500, detail=\"Failed to update session in database\")\n",
        "\n",
        "            except OperationFailure as e:\n",
        "                logger.error(f\"Database error updating session ID for user {user.username}: {e.details}\")\n",
        "                if session_id in cag_model.sessions: del cag_model.sessions[session_id]\n",
        "                raise HTTPException(status_code=500, detail=\"Database error during session update\")\n",
        "            except Exception as e:\n",
        "                logger.exception(f\"Unexpected error updating session ID for {user.username}\")\n",
        "                if session_id in cag_model.sessions: del cag_model.sessions[session_id]\n",
        "                raise HTTPException(status_code=500, detail=\"Internal server error during session update\")\n",
        "\n",
        "        if session_loaded_or_created:\n",
        "            logger.info(f\"Login successful for {user.username}. Returning CAG session token {session_id}\")\n",
        "            return LoginResponse(\n",
        "                username=found_user[\"username\"],\n",
        "                id=user_id_str,\n",
        "                session_id=session_id\n",
        "            )\n",
        "        else:\n",
        "            logger.error(f\"Login failed for {user.username} despite credentials being correct - CAG session could not be loaded or created.\")\n",
        "            raise HTTPException(status_code=500, detail=\"Internal server error during session handling\")\n",
        "\n",
        "    else:\n",
        "        logger.warning(f\"Login failed for user {user.username}: Invalid username or password.\")\n",
        "        raise HTTPException(status_code=401, detail=\"Invalid username or password\")\n",
        "\n",
        "@app.post(\"/logout/{user_id}\", response_model=LogoutResponse)\n",
        "async def logout_user(user_id: str, session_id: str = Depends(get_session_from_token)):\n",
        "    \"\"\"Đăng xuất người dùng.\"\"\"\n",
        "    users_collection = get_users_collection()\n",
        "    cag_model = get_cag_model()\n",
        "    logger.info(f\"Logout request for user ID: {user_id} with active session {session_id}\")\n",
        "\n",
        "    try:\n",
        "        if not ObjectId.is_valid(user_id):\n",
        "            logger.warning(f\"Logout failed: Invalid user ID format '{user_id}'\")\n",
        "            raise HTTPException(status_code=400, detail=\"Invalid user ID format\")\n",
        "\n",
        "        found_user = users_collection.find_one({\"_id\": ObjectId(user_id)})\n",
        "        username = \"N/A\"\n",
        "        if found_user:\n",
        "            username = found_user.get(\"username\", \"N/A\")\n",
        "            stored_session_id = found_user.get(\"session_id\")\n",
        "            if stored_session_id != session_id:\n",
        "                logger.warning(f\"User {username} (ID: {user_id}) logging out with token session {session_id}, but DB stores session {stored_session_id}. Proceeding with logout based on token.\")\n",
        "\n",
        "        # Optional: Xóa session khỏi memory\n",
        "        if session_id in cag_model.sessions:\n",
        "            del cag_model.sessions[session_id]\n",
        "            logger.info(f\"Removed session {session_id} from CAG model memory during logout for user {username}.\")\n",
        "            if cag_model.current_session_id == session_id:\n",
        "                cag_model.current_session_id = None\n",
        "                cag_model.conversation_history = []\n",
        "\n",
        "        logger.info(f\"User {username} (ID: {user_id}) logout processed successfully.\")\n",
        "        return LogoutResponse(message=\"Successfully logged out\")\n",
        "    except HTTPException as http_exc:\n",
        "        raise http_exc\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Error during logout processing for user ID {user_id}\")\n",
        "        return LogoutResponse(message=\"Logout processed with server error\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IjFnzbDQuf6",
        "outputId": "04b8739a-18fd-4332-f874-4566ae844415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_cag.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "from ipywidgets import HTML\n",
        "from IPython.display import display\n",
        "\n",
        "t = HTML(\n",
        "    value=\"0 Seconds\",\n",
        "    description='Server is Starting Up... Elapsed Time:',\n",
        "    style={'description_width': 'initial'},\n",
        ")\n",
        "display(t)\n",
        "\n",
        "flag = True\n",
        "timer = 0\n",
        "\n",
        "try:\n",
        "    subprocess.check_output(['curl', \"localhost:8000\"])\n",
        "    flag = False\n",
        "except:\n",
        "    # Khởi động server với chế độ reload\n",
        "    get_ipython().system_raw('uvicorn app_cag:app --host 0.0.0.0 --port 8000 --reload > server.log 2>&1 &')\n",
        "\n",
        "while flag and timer < 600:\n",
        "    try:\n",
        "        subprocess.check_output(['curl', \"localhost:8000\"])\n",
        "    except:\n",
        "        time.sleep(1)\n",
        "        timer += 1\n",
        "        t.value = str(timer) + \" Seconds\"\n",
        "    else:\n",
        "        flag = False\n",
        "\n",
        "if timer >= 600:\n",
        "    print(\"Error: timed out! Took more than 10 minutes :(\")\n",
        "\n",
        "subprocess.check_output(['curl', \"localhost:8000\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "ff4d6903e58d435a9f2b53c26bc53d3f",
            "5ecba2570b954f2ba0a7ccc7f7c8dd11",
            "9567641fa46245808bd08dbff97c2ff0"
          ]
        },
        "id": "bYJrD6gzRACC",
        "outputId": "cd851df8-4d03-4eb2-e6c4-8b70f435990d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='0 Seconds', description='Server is Starting Up... Elapsed Time:', style=DescriptionStyle(descripti…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff4d6903e58d435a9f2b53c26bc53d3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'{\"status\":\"CAG API is running\",\"gpu\":\"Available (Tesla T4)\"}'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lfDqSa_QuJWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This starts Ngrok and creates the public URL\n",
        "import subprocess\n",
        "import time\n",
        "import sys\n",
        "import json\n",
        "\n",
        "from IPython import get_ipython\n",
        "get_ipython().system_raw('ngrok http 8000 &')\n",
        "time.sleep(1)\n",
        "curlOut = subprocess.check_output(['curl',\"http://localhost:4040/api/tunnels\"],universal_newlines=True)\n",
        "time.sleep(1)\n",
        "ngrokURL = json.loads(curlOut)['tunnels'][0]['public_url']\n",
        "%store ngrokURL\n",
        "print(ngrokURL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QqasCrbRGqz",
        "outputId": "4d4e17d0-4e9a-4c7c-fbfb-956f649e811d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stored 'ngrokURL' (str)\n",
            "https://00c8-35-198-192-47.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "# Định nghĩa URL của endpoint FastAPI\n",
        "%store -r ngrokURL\n",
        "\n",
        "# Bước 1: Upload tài liệu PDF\n",
        "pdf_path = \"/content/LSVN_TomTat.pdf\"  # Thay đường dẫn tới file PDF của bạn\n",
        "with open(pdf_path, \"rb\") as f:\n",
        "    files = {\"file\": f}\n",
        "    upload_response = requests.post(ngrokURL + \"/upload/pdf\", files=files)\n",
        "\n",
        "if upload_response.status_code == 200:\n",
        "    upload_result = upload_response.json()\n",
        "    print(\"Upload thành công! Document ID:\", upload_result[\"document_id\"])\n",
        "else:\n",
        "    print(\"Upload thất bại với mã trạng thái:\", upload_response.status_code)\n",
        "\n",
        "    exit()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4cUwgcyRJ65",
        "outputId": "726dde1a-525e-44a4-bbc6-a9aa463ed376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload thành công! Document ID: 03ad90df-eab4-4cb2-b4b8-d2c410911f4d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i :8000"
      ],
      "metadata": {
        "id": "5fwh9DQJ7n3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 6643 3858 8460"
      ],
      "metadata": {
        "id": "J0ePFX12CXtp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24c2591a-5362-4f2a-ab81-e0dc166266c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: kill: (6643) - No such process\n",
            "/bin/bash: line 1: kill: (3858) - No such process\n",
            "/bin/bash: line 1: kill: (8460) - No such process\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "url = ngrokURL + \"/chat/noauth\"\n",
        "headers = {\n",
        "    'Content-Type': 'application/json'\n",
        "}\n",
        "\n",
        "session_id = None  # Biến để lưu trữ session_id\n",
        "\n",
        "while True:\n",
        "    query = input(\"Bạn: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        print(\"Kết thúc trò chuyện.\")\n",
        "        break\n",
        "\n",
        "    payload = {\n",
        "        \"query\": query\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            response_data = response.json()\n",
        "            print(\"Chatbot:\", response_data['response'])\n",
        "            if session_id is None:\n",
        "                session_id = response_data['session_id']  # Lưu session_id từ phản hồi đầu tiên\n",
        "        else:\n",
        "            print(\"Lỗi khi gửi tin nhắn. Mã trạng thái:\", response.status_code)\n",
        "            print(\"Chi tiết lỗi:\", response.text) # In chi tiết lỗi\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Lỗi kết nối: {e}\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Lỗi: Phản hồi từ server không phải là JSON hợp lệ.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi không mong muốn: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "id": "1wUDFfKD9J60",
        "outputId": "1d3d2c55-dfca-4590-a74b-d177371e5f72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bạn: chào\n",
            "Lỗi khi gửi tin nhắn. Mã trạng thái: 404\n",
            "Chi tiết lỗi: <!DOCTYPE html>\n",
            "<html class=\"h-full\" lang=\"en-US\" dir=\"ltr\">\n",
            "  <head>\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-Regular-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-RegularItalic-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-Medium-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-Semibold-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-MediumItalic-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/ibm-plex-mono/IBMPlexMono-Text.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/ibm-plex-mono/IBMPlexMono-TextItalic.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/ibm-plex-mono/IBMPlexMono-SemiBold.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/ibm-plex-mono/IBMPlexMono-SemiBoldItalic.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <meta charset=\"utf-8\">\n",
            "    <meta name=\"author\" content=\"ngrok\">\n",
            "    <meta name=\"description\" content=\"ngrok is the fastest way to put anything on the internet with a single command.\">\n",
            "    <meta name=\"robots\" content=\"noindex, nofollow\">\n",
            "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
            "    <link id=\"style\" rel=\"stylesheet\" href=\"https://cdn.ngrok.com/static/css/error.css\">\n",
            "    <noscript>The endpoint 00c8-35-198-192-47.ngrok-free.app is offline. (ERR_NGROK_3200)</noscript>\n",
            "    <script id=\"script\" src=\"https://cdn.ngrok.com/static/js/error.js\" type=\"text/javascript\"></script>\n",
            "  </head>\n",
            "  <body class=\"h-full\" id=\"ngrok\">\n",
            "    <div id=\"root\" data-payload=\"eyJjZG5CYXNlIjoiaHR0cHM6Ly9jZG4ubmdyb2suY29tLyIsImNvZGUiOiIzMjAwIiwibWVzc2FnZSI6IlRoZSBlbmRwb2ludCAwMGM4LTM1LTE5OC0xOTItNDcubmdyb2stZnJlZS5hcHAgaXMgb2ZmbGluZS4iLCJ0aXRsZSI6Ik5vdCBGb3VuZCJ9\"></div>\n",
            "  </body>\n",
            "</html>\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-5b48e18cc37b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bạn: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Kết thúc trò chuyện.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}