{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c2d4ed5a89024130a021ae5243e1f719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "Server is Starting Up... Elapsed Time:",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c976e072db46490cb7e081659745ef22",
            "placeholder": "​",
            "style": "IPY_MODEL_e57188ef0ec343a9b0efc38996503ac0",
            "value": "174 Seconds"
          }
        },
        "c976e072db46490cb7e081659745ef22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e57188ef0ec343a9b0efc38996503ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# If this complains about dependency resolver, it's safe to ignore\n",
        "!pip install fastapi[all] uvicorn python-multipart transformers pydantic tensorflow pymongo bcrypt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noIkv-nXAkpe",
        "outputId": "d231f783-5699-4781-92b0-243d06518721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting bcrypt\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Collecting fastapi[all]\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi[all])\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[all]) (4.13.2)\n",
            "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all])\n",
            "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[all]) (0.28.1)\n",
            "Requirement already satisfied: jinja2>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from fastapi[all]) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[all]) (2.2.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from fastapi[all]) (6.0.2)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi[all])\n",
            "  Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: orjson>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from fastapi[all]) (3.10.16)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[all])\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings>=2.0.0 (from fastapi[all])\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting pydantic-extra-types>=2.0.0 (from fastapi[all])\n",
            "  Downloading pydantic_extra_types-2.10.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[all]) (3.10)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (0.15.2)\n",
            "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all])\n",
            "  Downloading rich_toolkit-0.14.4-py3-none-any.whl.metadata (999 bytes)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[all]) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[all]) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[all]) (1.0.9)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.1.5->fastapi[all]) (3.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings>=2.0.0->fastapi[all])\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all])\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all])\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all])\n",
            "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all]) (15.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->fastapi[all]) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading pymongo-4.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
            "Downloading pydantic_extra_types-2.10.4-py3-none-any.whl (37 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading rich_toolkit-0.14.4-py3-none-any.whl (24 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvloop, uvicorn, ujson, python-multipart, python-dotenv, httptools, dnspython, bcrypt, watchfiles, starlette, pymongo, email-validator, rich-toolkit, pydantic-settings, pydantic-extra-types, fastapi, fastapi-cli\n",
            "Successfully installed bcrypt-4.3.0 dnspython-2.7.0 email-validator-2.2.0 fastapi-0.115.12 fastapi-cli-0.0.7 httptools-0.6.4 pydantic-extra-types-2.10.4 pydantic-settings-2.9.1 pymongo-4.12.1 python-dotenv-1.1.0 python-multipart-0.0.20 rich-toolkit-0.14.4 starlette-0.46.2 ujson-5.10.0 uvicorn-0.34.2 uvloop-0.21.0 watchfiles-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-openai openai faiss-cpu python-dotenv rank_bm25 bitsandbytes transformers accelerate\n",
        "!pip install -U langchain-community\n",
        "!pip install pymupdf\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goViY4WiGq-a",
        "outputId": "f564d46b-25f8-4116-916f-ba41de763c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.14-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.76.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.56)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.34)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading langchain_openai-0.3.14-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rank_bm25, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, langchain-openai, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.5 faiss-cpu-1.11.0 langchain-openai-0.3.14 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rank_bm25-0.2.2 tiktoken-0.9.0\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.56 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.56)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.24)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.34)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.56->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (2.33.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.23-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.23 marshmallow-3.26.1 mypy-extensions-1.1.0 typing-inspect-0.9.0\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.5\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-j2awau86\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-j2awau86\n",
            "  Resolved https://github.com/huggingface/transformers to commit 7a3e208892c06a5e278144eaf38c8599a42f53e7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.0.dev0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.0.dev0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (2025.1.31)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.52.0.dev0-py3-none-any.whl size=11588256 sha256=fde57707c4dc25f8151dafd7c92234a34926de7a76f603dd382802caad5431ab\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g1vh7krx/wheels/04/a3/f1/b88775f8e1665827525b19ac7590250f1038d947067beba9fb\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "Successfully installed transformers-4.52.0.dev0\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8Mi_3Cw4Gsw2",
        "outputId": "ed5ac3af-194b-4a08-f589-816e2b9049f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vllm\n",
            "  Downloading vllm-0.8.5-cp38-abi3-manylinux1_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vllm) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm) (4.67.1)\n",
            "Collecting blake3 (from vllm)\n",
            "  Downloading blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.51.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.52.0.dev0)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[hf_xet]>=0.30.0->vllm) (0.30.2)\n",
            "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (5.29.4)\n",
            "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.115.12)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.11.15)\n",
            "Requirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.76.0)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.11.3)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.2.1)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
            "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\n",
            "Collecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n",
            "  Downloading lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting llguidance<0.8.0,>=0.7.9 (from vllm)\n",
            "  Downloading llguidance-0.7.19-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting outlines==0.1.11 (from vllm)\n",
            "  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting lark==1.2.2 (from vllm)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xgrammar==0.1.18 (from vllm)\n",
            "  Downloading xgrammar-0.1.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.13.2)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (3.18.0)\n",
            "Collecting partial-json-parser (from vllm)\n",
            "  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting pyzmq>=25.0.0 (from vllm)\n",
            "  Downloading pyzmq-26.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting msgspec (from vllm)\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting gguf>=0.13.0 (from vllm)\n",
            "  Downloading gguf-0.16.2-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from vllm) (8.6.1)\n",
            "Collecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm)\n",
            "  Downloading mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.11.0.86)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.1)\n",
            "Collecting compressed-tensors==0.9.3 (from vllm)\n",
            "  Downloading compressed_tensors-0.9.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting depyf==0.18.0 (from vllm)\n",
            "  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\n",
            "Requirement already satisfied: watchfiles in /usr/local/lib/python3.11/dist-packages (from vllm) (1.0.5)\n",
            "Collecting python-json-logger (from vllm)\n",
            "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.15.2)\n",
            "Collecting ninja (from vllm)\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting opentelemetry-sdk<1.27.0,>=1.26.0 (from vllm)\n",
            "  Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api<1.27.0,>=1.26.0 (from vllm)\n",
            "  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp<1.27.0,>=1.26.0 (from vllm)\n",
            "  Downloading opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions-ai<0.5.0,>=0.4.1 (from vllm)\n",
            "  Downloading opentelemetry_semantic_conventions_ai-0.4.5-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting numba==0.61.2 (from vllm)\n",
            "  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting ray!=2.44.*,>=2.43.0 (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
            "  Downloading ray-2.45.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio==2.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision==0.21.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.0+cu124)\n",
            "Collecting xformers==0.0.29.post2 (from vllm)\n",
            "  Downloading xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting astor (from depyf==0.18.0->vllm)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting dill (from depyf==0.18.0->vllm)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n",
            "  Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting interegular (from outlines==0.1.11->vllm)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (3.1.6)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
            "Collecting diskcache (from outlines==0.1.11->vllm)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.36.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
            "Collecting pycountry (from outlines==0.1.11->vllm)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting airportsdata (from outlines==0.1.11->vllm)\n",
            "  Downloading airportsdata-20250224-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n",
            "  Downloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->vllm) (1.3.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.46.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.7)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.2.0)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.34.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.0->huggingface-hub[hf_xet]>=0.30.0->vllm) (24.2)\n",
            "Collecting hf-xet>=0.1.4 (from huggingface-hub[hf_xet]>=0.30.0->vllm)\n",
            "  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.3.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<1.27.0,>=1.26.0->vllm) (1.2.18)\n",
            "Collecting importlib_metadata (from vllm)\n",
            "  Downloading importlib_metadata-8.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->vllm) (3.21.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.71.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting protobuf (from vllm)\n",
            "  Downloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-sdk<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.4.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.1.8)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.0)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (13.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2025.1.31)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.6.0->vllm) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.1->vllm) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.20.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<1.27.0,>=1.26.0->vllm) (1.17.2)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.2)\n",
            "Requirement already satisfied: rich-toolkit>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.14.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2025.4.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.24.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.1.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (0.8.3)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (13.9.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
            "Downloading vllm-0.8.5-cp38-abi3-manylinux1_x86_64.whl (326.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.4/326.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.9.3-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.4/98.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading depyf-0.18.0-py3-none-any.whl (38 kB)\n",
            "Downloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines-0.1.11-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl (44.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgrammar-0.1.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.16.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llguidance-0.7.19-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.5.4-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\n",
            "Downloading opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.26.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions_ai-0.4.5-py3-none-any.whl (5.5 kB)\n",
            "Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyzmq-26.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (862 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.4/862.4 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.45.0-cp311-cp311-manylinux2014_x86_64.whl (68.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
            "Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading airportsdata-20250224-py3-none-any.whl (913 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: blake3, pyzmq, python-json-logger, pycountry, protobuf, partial-json-parser, opentelemetry-semantic-conventions-ai, ninja, msgspec, llvmlite, llguidance, lark, interegular, importlib_metadata, hf-xet, gguf, diskcache, dill, astor, airportsdata, opentelemetry-proto, opentelemetry-api, numba, depyf, prometheus-fastapi-instrumentator, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, lm-format-enforcer, xformers, ray, outlines_core, opentelemetry-sdk, mistral_common, xgrammar, outlines, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, compressed-tensors, opentelemetry-exporter-otlp, vllm\n",
            "  Attempting uninstall: pyzmq\n",
            "    Found existing installation: pyzmq 24.0.1\n",
            "    Uninstalling pyzmq-24.0.1:\n",
            "      Successfully uninstalled pyzmq-24.0.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib_metadata 8.6.1\n",
            "    Uninstalling importlib_metadata-8.6.1:\n",
            "      Successfully uninstalled importlib_metadata-8.6.1\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.32.1\n",
            "    Uninstalling opentelemetry-api-1.32.1:\n",
            "      Successfully uninstalled opentelemetry-api-1.32.1\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.60.0\n",
            "    Uninstalling numba-0.60.0:\n",
            "      Successfully uninstalled numba-0.60.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.53b1\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.53b1:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.53b1\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.32.1\n",
            "    Uninstalling opentelemetry-sdk-1.32.1:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.32.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "google-cloud-pubsub 2.29.0 requires opentelemetry-api>=1.27.0; python_version >= \"3.8\", but you have opentelemetry-api 1.26.0 which is incompatible.\n",
            "google-cloud-pubsub 2.29.0 requires opentelemetry-sdk>=1.27.0; python_version >= \"3.8\", but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed airportsdata-20250224 astor-0.8.1 blake3-1.0.4 compressed-tensors-0.9.3 depyf-0.18.0 dill-0.4.0 diskcache-5.6.3 gguf-0.16.2 hf-xet-1.1.0 importlib_metadata-8.0.0 interegular-0.3.3 lark-1.2.2 llguidance-0.7.19 llvmlite-0.44.0 lm-format-enforcer-0.10.11 mistral_common-1.5.4 msgspec-0.19.0 ninja-1.11.1.4 numba-0.61.2 opentelemetry-api-1.26.0 opentelemetry-exporter-otlp-1.26.0 opentelemetry-exporter-otlp-proto-common-1.26.0 opentelemetry-exporter-otlp-proto-grpc-1.26.0 opentelemetry-exporter-otlp-proto-http-1.26.0 opentelemetry-proto-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 opentelemetry-semantic-conventions-ai-0.4.5 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post5 prometheus-fastapi-instrumentator-7.1.0 protobuf-4.25.7 pycountry-24.6.1 python-json-logger-3.3.0 pyzmq-26.4.0 ray-2.45.0 vllm-0.8.5 xformers-0.0.29.post2 xgrammar-0.1.18\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "importlib_metadata"
                ]
              },
              "id": "44918f3b4970434d9adb79087213810d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This downloads and sets up the Ngrok executable in the Google Colab instance\n",
        "# Import the ngrok GPG key\n",
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | gpg --import -\n",
        "\n",
        "# Add the ngrok repository to the apt sources list\n",
        "!echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list\n",
        "\n",
        "# Fetch the public key associated with the ngrok repository\n",
        "!sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 0E61D3BBAAEE37FE\n",
        "\n",
        "# Update the apt package lists\n",
        "!sudo apt-get update\n",
        "\n",
        "# Install ngrok\n",
        "!sudo apt-get install ngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH_wH8XWGspf",
        "outputId": "a87531e8-3401-4cf9-abc6-95e67d0cda30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpg: key 0E61D3BBAAEE37FE: \"ngrok agent apt repo release bot <release-bot@ngrok.com>\" not changed\n",
            "gpg: Total number processed: 1\n",
            "gpg:              unchanged: 1\n",
            "deb https://ngrok-agent.s3.amazonaws.com buster main\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "Executing: /tmp/apt-key-gpghome.ZVxz3UbjOI/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 0E61D3BBAAEE37FE\n",
            "gpg: key 0E61D3BBAAEE37FE: public key \"ngrok agent apt repo release bot <release-bot@ngrok.com>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:3 https://ngrok-agent.s3.amazonaws.com buster InRelease [20.3 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://ngrok-agent.s3.amazonaws.com buster/main amd64 Packages [8,075 B]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,607 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,706 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,877 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,244 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,844 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,544 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,155 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,310 kB]\n",
            "Fetched 26.8 MB in 7s (3,999 kB/s)\n",
            "Reading package lists... Done\n",
            "W: https://ngrok-agent.s3.amazonaws.com/dists/buster/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  ngrok\n",
            "0 upgraded, 1 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 6,777 kB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 https://ngrok-agent.s3.amazonaws.com buster/main amd64 ngrok amd64 3.22.1 [6,777 kB]\n",
            "Fetched 6,777 kB in 0s (18.3 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package ngrok.\n",
            "(Reading database ... 126101 files and directories currently installed.)\n",
            "Preparing to unpack .../ngrok_3.22.1_amd64.deb ...\n",
            "Unpacking ngrok (3.22.1) ...\n",
            "Setting up ngrok (3.22.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://dashboard.ngrok.com/signup\n",
        "!ngrok authtoken 2uZINKAUETNXWONds7xDf4VDfBT_7M7FgTzgKVxByGNy1EtBf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsIBRSPjRDA0",
        "outputId": "23958c8e-7479-47e1-c0dc-93af154a87e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cag.py\n",
        "from typing import List, Tuple, Dict\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from langchain.docstore.document import Document # Vẫn giữ nếu các phương thức khác dùng Document object\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "# Removed unused transformers imports for model loading (using vLLM)\n",
        "from transformers import AutoTokenizer\n",
        "# Removed BitsAndBytesConfig, DynamicCache, TextStreamer\n",
        "import re\n",
        "import uuid\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "import time\n",
        "import PyPDF2\n",
        "# Removed TokenTextSplitter if not explicitly used in chunking anymore\n",
        "# from langchain.text_splitter import TokenTextSplitter\n",
        "\n",
        "# Import vLLM\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Removed safe globals for DynamicCache as it's no longer used\n",
        "# torch.serialization.add_safe_globals([DynamicCache])\n",
        "# torch.serialization.add_safe_globals([set])\n",
        "\n",
        "# PyMuPDF requires separate installation - add a check/import\n",
        "try:\n",
        "    import fitz # PyMuPDF\n",
        "    HAS_PYMUPDF = True\n",
        "    logger.info(\"PyMuPDF is available.\")\n",
        "except ImportError:\n",
        "    HAS_PYMUPDF = False\n",
        "    logger.warning(\"PyMuPDF not found. Falling back to PyPDF2 for PDF processing.\")\n",
        "\n",
        "\n",
        "class ContextualCAG:\n",
        "\n",
        "    def __init__(self, conversation_memory_size=5):\n",
        "        \"\"\"\n",
        "        Khởi tạo hệ thống CAG có tính ngữ cảnh cuộc hội thoại sử dụng vLLM\n",
        "\n",
        "        Args:\n",
        "            conversation_memory_size: Số lượng cuộc hội thoại gần nhất được lưu trữ\n",
        "        \"\"\"\n",
        "        # vLLM manages device placement automatically, but we keep this for other ops\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        logger.info(f\"Using device for non-LLM ops: {self.device}\")\n",
        "\n",
        "        # Model names\n",
        "        self.llm_model_name = \"AITeamVN/Vi-Qwen2-3B-RAG\"\n",
        "        self.encoder_model_name = \"bkai-foundation-models/vietnamese-bi-encoder\"\n",
        "\n",
        "        # Load encoder model for embedding (used in filter/rerank, though unused in main flow now)\n",
        "        logger.info(f\"Loading encoder model: {self.encoder_model_name}\")\n",
        "        self.encoder = SentenceTransformer(self.encoder_model_name)\n",
        "\n",
        "        # Load LLM using vLLM\n",
        "        logger.info(f\"Loading LLM model with vLLM: {self.llm_model_name}\")\n",
        "        # vLLM handles quantization and device placement internally\n",
        "        # dtype=\"half\" is for FP16. Use quantization=\"awq\" or \"gptq\" if the model ID specifically supports it\n",
        "        # gpu_memory_utilization can be adjusted based on your GPU\n",
        "        try:\n",
        "            self.llm = LLM(\n",
        "                model=self.llm_model_name,\n",
        "                dtype=\"half\", # Or \"auto\", or specific quantization like \"awq\" if supported\n",
        "                gpu_memory_utilization=0.9, # Adjust as needed\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            logger.info(\"vLLM model loaded successfully.\")\n",
        "            # vLLM uses its own tokenizer, but we might keep the HF tokenizer for chat template logic\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.llm_model_name)\n",
        "            logger.info(\"Tokenizer loaded successfully.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load LLM model with vLLM: {e}\")\n",
        "            self.llm = None\n",
        "            self.tokenizer = None\n",
        "            # Consider exiting or raising an error if model loading fails\n",
        "\n",
        "        # Conversation tracking\n",
        "        self.conversation_history: List[Dict[str, str]] = [] # History for current session limited by memory_size\n",
        "        self.conversation_memory_size = conversation_memory_size\n",
        "        self.current_session_id: str | None = None\n",
        "        self.sessions: Dict[str, List[Dict[str, str]]] = {} # Full history across sessions\n",
        "\n",
        "        # Store the processed document chunks\n",
        "        self.knowledge_chunks: List[str] = []\n",
        "\n",
        "        # Setup reranker if GPU is available (optional, currently unused in main flow)\n",
        "        self.has_reranker = False\n",
        "        # Reranker logic remains the same if needed for other purposes\n",
        "        if torch.cuda.is_available():\n",
        "             try:\n",
        "                 self.reranker = SentenceTransformer('intfloat/multilingual-e5-large')\n",
        "                 self.reranker.to(self.device)\n",
        "                 self.has_reranker = True\n",
        "                 logger.info(\"Reranker model loaded.\")\n",
        "             except Exception as e:\n",
        "                 self.has_reranker = False\n",
        "                 logger.warning(f\"Could not load reranker model: {e}\")\n",
        "        else:\n",
        "             logger.warning(\"GPU not available. Reranker model not loaded.\")\n",
        "\n",
        "\n",
        "    def start_new_session(self):\n",
        "        \"\"\"\n",
        "        Bắt đầu một phiên hội thoại mới\n",
        "\n",
        "        Returns:\n",
        "            str: ID phiên mới\n",
        "        \"\"\"\n",
        "        self.current_session_id = str(uuid.uuid4())\n",
        "        self.sessions[self.current_session_id] = []\n",
        "        self.conversation_history = [] # Clear history for the new session\n",
        "        logger.info(f\"Started new session: {self.current_session_id}\")\n",
        "        return self.current_session_id\n",
        "\n",
        "    def load_session(self, session_id):\n",
        "        \"\"\"\n",
        "        Tải một phiên hội thoại đã lưu\n",
        "\n",
        "        Args:\n",
        "            session_id: ID phiên cần tải\n",
        "\n",
        "        Returns:\n",
        "            bool: True nếu tải thành công, False nếu không\n",
        "        \"\"\"\n",
        "        if session_id in self.sessions:\n",
        "            self.current_session_id = session_id\n",
        "            # Load history, limited by memory size\n",
        "            self.conversation_history = self.sessions[session_id][-self.conversation_memory_size:]\n",
        "            logger.info(f\"Loaded session: {self.current_session_id}\")\n",
        "            return True\n",
        "        logger.warning(f\"Session ID not found: {session_id}\")\n",
        "        return False\n",
        "\n",
        "    def add_to_conversation(self, query, response):\n",
        "        \"\"\"\n",
        "        Thêm một cặp hỏi đáp vào lịch sử cuộc trò chuyện\n",
        "\n",
        "        Args:\n",
        "            query: Câu hỏi của người dùng\n",
        "            response: Câu trả lời của hệ thống\n",
        "        \"\"\"\n",
        "        if self.current_session_id is None:\n",
        "            self.start_new_session()\n",
        "\n",
        "        conv_pair = {\"query\": query, \"response\": response}\n",
        "\n",
        "        # Add to the full session history\n",
        "        self.sessions[self.current_session_id].append(conv_pair)\n",
        "\n",
        "        # Update the active conversation history (limited by memory size)\n",
        "        self.conversation_history = self.sessions[self.current_session_id][-self.conversation_memory_size:]\n",
        "        logger.debug(f\"Added to conversation history. Current length: {len(self.conversation_history)}\")\n",
        "\n",
        "\n",
        "    def load_pdf(self, pdf_path):\n",
        "        \"\"\"\n",
        "        Đọc và xử lý tệp PDF thành các đoạn văn bản\n",
        "\n",
        "        Args:\n",
        "            pdf_path: Đường dẫn đến tệp PDF\n",
        "\n",
        "        Returns:\n",
        "            list: Danh sách các đoạn văn bản từ PDF\n",
        "        \"\"\"\n",
        "        logger.info(f\"Reading PDF: {pdf_path}\")\n",
        "        chunks = []\n",
        "\n",
        "        if not os.path.exists(pdf_path):\n",
        "            logger.error(f\"PDF file not found: {pdf_path}\")\n",
        "            return chunks\n",
        "\n",
        "        try:\n",
        "            if HAS_PYMUPDF:\n",
        "                try:\n",
        "                    doc = fitz.open(pdf_path)\n",
        "                    for page_num in range(len(doc)):\n",
        "                        page = doc.load_page(page_num)\n",
        "                        text = page.get_text()\n",
        "\n",
        "                        # Tách thành các đoạn văn bản\n",
        "                        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
        "                        for para in paragraphs:\n",
        "                            if len(para.strip()) > 0:\n",
        "                                chunks.append(para.strip())\n",
        "                    doc.close()\n",
        "                    logger.info(f\"Extracted {len(chunks)} text chunks using PyMuPDF.\")\n",
        "                    return chunks\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error using PyMuPDF: {e}. Falling back to PyPDF2.\")\n",
        "\n",
        "            # Phương pháp 2: Sử dụng PyPDF2 (backup)\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "                # Xử lý từng trang\n",
        "                for page_num in range(len(reader.pages)):\n",
        "                    page = reader.pages[page_num]\n",
        "                    text = page.extract_text()\n",
        "\n",
        "                    if text:\n",
        "                        # Tách thành các đoạn văn bản\n",
        "                        # Simplified split, consider more robust splitting if needed\n",
        "                        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
        "                        for para in paragraphs:\n",
        "                            if len(para.strip()) > 0:\n",
        "                                chunks.append(para.strip())\n",
        "\n",
        "            logger.info(f\"Extracted {len(chunks)} text chunks using PyPDF2.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error reading or processing PDF {pdf_path}: {e}\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def compress_documents(self, docs: List[str], compression_ratio=0.7) -> List[str]:\n",
        "        \"\"\"\n",
        "        Nén các đoạn văn bản để giảm kích thước.\n",
        "        Lưu ý: Đây là một phương pháp nén rất đơn giản chỉ giữ lại câu đầu và câu cuối\n",
        "        cùng một vài câu ở giữa. Có thể không hiệu quả cho mọi loại văn bản.\n",
        "\n",
        "        Args:\n",
        "            docs: Danh sách các đoạn văn bản (string)\n",
        "            compression_ratio: Tỷ lệ nén (0-1) - tỷ lệ câu giữ lại\n",
        "\n",
        "        Returns:\n",
        "            list: Danh sách đoạn văn bản đã nén (string)\n",
        "        \"\"\"\n",
        "        compressed_docs = []\n",
        "\n",
        "        for doc in docs:\n",
        "            text = doc # docs are expected to be strings now\n",
        "\n",
        "            # Clean up spacing and newlines within paragraphs\n",
        "            text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces with single\n",
        "            # Decide if you want to keep single newlines as breaks within sentences\n",
        "            # text = re.sub(r'\\n+', '\\n', text) # Or replace multiple newlines with single\n",
        "\n",
        "            # Splitting by '.' might be problematic with abbreviations, etc.\n",
        "            # A more robust sentence splitter might be needed for better compression.\n",
        "            sentences = text.split('.')\n",
        "            sentences = [s.strip() for s in sentences if s.strip()] # Remove empty sentences\n",
        "\n",
        "            if len(sentences) <= 3:\n",
        "                compressed_text = '. '.join(sentences) # Join back with period and space\n",
        "            else:\n",
        "                num_sentences = max(3, int(len(sentences) * compression_ratio))\n",
        "                selected_sentences = [sentences[0]] # Always keep the first sentence\n",
        "\n",
        "                if num_sentences > 2: # Need space for middle sentences\n",
        "                    middle_sentences = sentences[1:-1]\n",
        "                    if middle_sentences: # Ensure there are middle sentences\n",
        "                         # Select middle sentences, distributing roughly evenly\n",
        "                        step = max(1, len(middle_sentences) // (num_sentences - 2))\n",
        "                        # Ensure we don't select more than available middle sentences\n",
        "                        selected_middle = middle_sentences[::step]\n",
        "                        selected_sentences.extend(selected_middle[:num_sentences-2])\n",
        "\n",
        "\n",
        "                if len(sentences) > 1:\n",
        "                   # Always keep the last sentence if there's more than one sentence\n",
        "                   selected_sentences.append(sentences[-1])\n",
        "\n",
        "\n",
        "                # Ensure uniqueness and maintain approximate order (optional but good)\n",
        "                # For simplicity, just join selected sentences in their original relative order\n",
        "                # Rebuild compressed text from selected sentences\n",
        "                compressed_text = '. '.join(selected_sentences)\n",
        "\n",
        "            compressed_docs.append(compressed_text)\n",
        "\n",
        "        logger.info(f\"Compressed {len(docs)} documents into {len(compressed_docs)} with simpler logic.\")\n",
        "        return compressed_docs\n",
        "\n",
        "    # filter_documents and rerank_documents are not used in the main vLLM chat flow,\n",
        "    # as vLLM takes the full context. Keeping them in case they are needed for other purposes.\n",
        "    def filter_documents(self, chunks: List[str], query: str, threshold=0.3) -> List[str]:\n",
        "         \"\"\"\n",
        "         Lọc các đoạn văn bản theo độ liên quan với câu hỏi (sử dụng self.encoder).\n",
        "         Không được dùng trong luồng vLLM chính.\n",
        "         \"\"\"\n",
        "         if not chunks:\n",
        "             return []\n",
        "\n",
        "         logger.debug(f\"Filtering {len(chunks)} documents...\")\n",
        "         query_embedding = self.encoder.encode(query, convert_to_tensor=True)\n",
        "\n",
        "         # Compute embeddings for all chunks\n",
        "         chunk_embeddings = self.encoder.encode(chunks, convert_to_tensor=True)\n",
        "\n",
        "         # Compute cosine similarity between query and all chunks\n",
        "         similarities = util.cos_sim(query_embedding, chunk_embeddings)[0] # shape is [num_chunks]\n",
        "\n",
        "         # Pair chunks with their scores and sort\n",
        "         chunk_score_pairs = list(zip(chunks, similarities.cpu().numpy()))\n",
        "         sorted_pairs = sorted(chunk_score_pairs, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "         # Filter based on threshold\n",
        "         filtered_chunks = [chunk for chunk, sim in sorted_pairs if sim >= threshold]\n",
        "\n",
        "         # If no chunks meet the threshold, return the single most similar chunk\n",
        "         if not filtered_chunks and sorted_pairs:\n",
        "             filtered_chunks = [sorted_pairs[0][0]]\n",
        "             logger.debug(f\"No chunks met threshold {threshold}. Returning top 1.\")\n",
        "         else:\n",
        "             logger.debug(f\"Filtered down to {len(filtered_chunks)} chunks above threshold {threshold}.\")\n",
        "\n",
        "\n",
        "         return filtered_chunks\n",
        "\n",
        "\n",
        "    def rerank_documents(self, chunks: List[str], query: str, top_k=5) -> List[str]:\n",
        "         \"\"\"\n",
        "         Xếp hạng lại các đoạn văn bản theo độ liên quan (sử dụng self.reranker nếu có).\n",
        "         Không được dùng trong luồng vLLM chính.\n",
        "         \"\"\"\n",
        "         if not self.has_reranker or not chunks:\n",
        "             logger.debug(\"Reranker not available or no chunks. Skipping reranking.\")\n",
        "             return chunks[:top_k] if len(chunks) > top_k else chunks\n",
        "\n",
        "         logger.debug(f\"Reranking {len(chunks)} documents...\")\n",
        "         reranker_query = f\"query: {query}\"\n",
        "\n",
        "         # Encode query and passages\n",
        "         query_embedding = self.reranker.encode([reranker_query], convert_to_tensor=True)\n",
        "         passage_embeddings = self.reranker.encode(chunks, convert_to_tensor=True)\n",
        "\n",
        "         # Compute scores\n",
        "         scores = util.cos_sim(query_embedding, passage_embeddings)[0] # shape is [num_chunks]\n",
        "\n",
        "         # Pair chunks with scores and sort\n",
        "         chunk_score_pairs = list(zip(chunks, scores.cpu().numpy()))\n",
        "         ranked_pairs = sorted(chunk_score_pairs, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "         # Return top_k\n",
        "         reranked_chunks = [pair[0] for pair in ranked_pairs[:top_k]]\n",
        "         logger.debug(f\"Reranked to top {len(reranked_chunks)} chunks.\")\n",
        "\n",
        "         return reranked_chunks\n",
        "\n",
        "\n",
        "    def preprocess_knowledge_prefix(self, text_chunks: List[str], answer_instruction: str | None = None) -> str:\n",
        "        \"\"\"\n",
        "        Chuẩn bị phần prompt chứa kiến thức từ tài liệu.\n",
        "        Phần này sẽ được kết hợp với lịch sử chat và câu hỏi hiện tại.\n",
        "\n",
        "        Args:\n",
        "            text_chunks: Danh sách các đoạn văn bản (string)\n",
        "            answer_instruction: Hướng dẫn cụ thể cho AI về cách trả lời\n",
        "\n",
        "        Returns:\n",
        "            str: Chuỗi prompt chứa kiến thức đã định dạng\n",
        "        \"\"\"\n",
        "        if answer_instruction is None:\n",
        "            answer_instruction = \"Hãy trả lời câu hỏi dựa trên thông tin bối cảnh được cung cấp. Nếu câu hỏi không liên quan trực tiếp đến thông tin, hãy sử dụng kiến thức chung của bạn.\"\n",
        "\n",
        "        # Kết hợp các chunk với dấu phân cách rõ ràng\n",
        "        documents = '\\n\\n---\\n\\n'.join(text_chunks) # Using a clearer separator\n",
        "\n",
        "        # Định dạng tiền tố prompt kiến thức\n",
        "        # We build the initial system/user block containing the knowledge\n",
        "        # The final prompt will be:\n",
        "        # <|im_start|>system\\n{system_message}<|im_end|>\n",
        "        # <|im_start|>user\\nThông tin bối cảnh...\\n---\\nLịch sử trò chuyện...\\nCâu hỏi: ...<|im_end|>\n",
        "        # <|im_start|>assistant\\n\n",
        "\n",
        "        system_message = f\"Bạn là trợ lý AI cung cấp câu trả lời. {answer_instruction}\"\n",
        "\n",
        "        # The knowledge part is presented as context within the first user turn\n",
        "        knowledge_context_string = f\"Thông tin bối cảnh được cung cấp bên dưới.\\n---\\n{documents}\\n---\"\n",
        "\n",
        "        return system_message, knowledge_context_string\n",
        "\n",
        "\n",
        "    # Removed prepare_kv_cache, clean_up_cache, read_kv_cache as vLLM handles this\n",
        "\n",
        "    def generate_conversation_context(self) -> str:\n",
        "        \"\"\"\n",
        "        Tạo ngữ cảnh từ lịch sử cuộc trò chuyện theo định dạng text đơn giản.\n",
        "\n",
        "        Returns:\n",
        "            str: Chuỗi text lịch sử trò chuyện hoặc chuỗi rỗng\n",
        "        \"\"\"\n",
        "        if not self.conversation_history:\n",
        "            return \"\"\n",
        "\n",
        "        context = \"Lịch sử trò chuyện gần đây:\\n\"\n",
        "        for conv in self.conversation_history:\n",
        "            context += f\"Người dùng: {conv['query']}\\n\"\n",
        "            context += f\"Trợ lý: {conv['response']}\\n\"\n",
        "        context += \"---\" # Add a separator\n",
        "\n",
        "        return context\n",
        "\n",
        "\n",
        "    def enrich_query_with_context(self, query):\n",
        "        \"\"\"\n",
        "        Phương thức này có thể được sử dụng để phân tích query,\n",
        "        nhưng việc kết hợp ngữ cảnh sẽ được xử lý trực tiếp trong prompt.\n",
        "        Giữ lại nếu muốn phân tích ý định sâu hơn sau này.\n",
        "        \"\"\"\n",
        "        # Currently no change to the query text itself\n",
        "        return query\n",
        "\n",
        "    def analyze_query_intent(self, query):\n",
        "        \"\"\"\n",
        "        Phân tích ý định của câu truy vấn (chỉ mang tính chất gợi ý, không ảnh hưởng luồng chính)\n",
        "        Giữ lại nếu muốn sử dụng cho logic phức tạp hơn sau đây.\n",
        "        \"\"\"\n",
        "        # (Implementation remains the same as it's intent analysis, not generation)\n",
        "        continue_patterns = [\"tiếp tục\", \"thêm về\", \"nói thêm\", \"giải thích thêm\", \"nữa\", \"về điều đó\"]\n",
        "        new_topic_patterns = [\"chủ đề mới\", \"câu hỏi khác\", \"nói về\", \"giờ hỏi về\", \"sang chủ đề\"]\n",
        "        greeting_patterns = [\"chào\", \"xin chào\", \"alo\", \"hi\"]\n",
        "        general_question_patterns = [\"bạn là ai\", \"bạn làm được gì\", \"thời tiết hôm nay\", \"hôm nay thế nào\", \"cảm ơn\", \"tạm biệt\"]\n",
        "\n",
        "        is_continuation = any(pattern in query.lower() for pattern in continue_patterns)\n",
        "        is_new_topic = any(pattern in query.lower() for pattern in new_topic_patterns)\n",
        "        is_greeting = any(pattern in query.lower() for pattern in greeting_patterns)\n",
        "        # Simple heuristic for general/non-document questions\n",
        "        is_general = (len(query.split()) < 6 or\n",
        "                      is_greeting or\n",
        "                      any(pattern in query.lower() for pattern in general_question_patterns))\n",
        "\n",
        "        # A query is likely related to the document if it's not clearly a greeting/general/new topic\n",
        "        # This simple logic might need refinement\n",
        "        is_document_related = not (is_greeting or is_general or is_new_topic)\n",
        "\n",
        "        return {\n",
        "            \"is_continuation\": is_continuation,\n",
        "            \"is_new_topic\": is_new_topic,\n",
        "            \"is_greeting\": is_greeting,\n",
        "            \"is_general\": is_general,\n",
        "            \"is_document_related\": is_document_related # Added based on simple heuristic\n",
        "        }\n",
        "\n",
        "\n",
        "    def answer_question(self, full_prompt: str, max_new_tokens: int = 300) -> str:\n",
        "        \"\"\"\n",
        "        Tạo câu trả lời bằng vLLM từ prompt đầy đủ.\n",
        "\n",
        "        Args:\n",
        "            full_prompt: Chuỗi prompt hoàn chỉnh (knowledge + history + query + assistant tag)\n",
        "            max_new_tokens: Số lượng token mới tối đa cần tạo\n",
        "\n",
        "        Returns:\n",
        "            str: Văn bản đã tạo\n",
        "        \"\"\"\n",
        "        if self.llm is None:\n",
        "            return \"Mô hình LLM chưa được tải thành công.\"\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Define sampling parameters\n",
        "        sampling_params = SamplingParams(\n",
        "            temperature=0.0, # Use 0.0 for greedy decoding similar to original code\n",
        "            top_p=1.0,\n",
        "            max_tokens=max_new_tokens,\n",
        "            stop=[\"<|im_end|>\"] # Stop generation when the end tag is encountered\n",
        "            # Add other parameters as needed (e.g., presence_penalty, frequency_penalty)\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            # vLLM generates text directly\n",
        "            # The input is the full prompt string\n",
        "            outputs = self.llm.generate(full_prompt, sampling_params)\n",
        "\n",
        "            # Extract generated text from the output\n",
        "            # outputs is a list of RequestOutput objects (usually one for a single prompt)\n",
        "            if outputs and outputs[0].outputs:\n",
        "                # Assuming we only generate one sequence per prompt\n",
        "                generated_text = outputs[0].outputs[0].text\n",
        "            else:\n",
        "                generated_text = \"Không thể tạo ra câu trả lời.\"\n",
        "                logger.warning(\"vLLM generate returned no output.\")\n",
        "\n",
        "            # Clean up the generated text if necessary (vLLM should stop at <|im_end|>)\n",
        "            # The stop sequence helps, but sometimes extra whitespace or tags might appear.\n",
        "            assistant_tag = \"<|im_start|>assistant\\n\"\n",
        "            end_tag = \"<|im_end|>\"\n",
        "\n",
        "            # Remove the assistant tag if it somehow appears at the start of generated text\n",
        "            # (vLLM usually doesn't generate the *input* assistant tag)\n",
        "            if generated_text.startswith(assistant_tag):\n",
        "                 generated_text = generated_text[len(assistant_tag):]\n",
        "\n",
        "            # Remove the end tag if it's still present at the end\n",
        "            if generated_text.endswith(end_tag):\n",
        "                generated_text = generated_text[:-len(end_tag)]\n",
        "\n",
        "            # Basic stripping of leading/trailing whitespace\n",
        "            generated_text = generated_text.strip()\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during vLLM generation: {e}\")\n",
        "            generated_text = \"Có lỗi xảy ra khi tạo câu trả lời.\"\n",
        "\n",
        "\n",
        "        generation_time = time.time() - start_time\n",
        "        logger.info(f\"Response generated in {generation_time:.2f} seconds using vLLM\")\n",
        "\n",
        "        return generated_text\n",
        "\n",
        "\n",
        "    def process_pdf_document(self, pdf_path, use_compression=True):\n",
        "        \"\"\"\n",
        "        Xử lý tài liệu PDF thành các đoạn văn bản và lưu vào self.knowledge_chunks.\n",
        "        Không tạo KV cache ở đây nữa.\n",
        "\n",
        "        Args:\n",
        "            pdf_path: Đường dẫn đến tệp PDF\n",
        "            use_compression: Có nén các đoạn văn bản hay không\n",
        "        \"\"\"\n",
        "        if self.llm is None or self.tokenizer is None:\n",
        "             logger.error(\"LLM or Tokenizer not loaded. Cannot process document.\")\n",
        "             return False\n",
        "\n",
        "        logger.info(f\"Processing document: {pdf_path}\")\n",
        "        chunks = self.load_pdf(pdf_path)\n",
        "\n",
        "        if not chunks:\n",
        "            logger.warning(\"No chunks extracted from PDF.\")\n",
        "            self.knowledge_chunks = []\n",
        "            return False\n",
        "\n",
        "        if use_compression:\n",
        "            logger.info(\"Applying document compression.\")\n",
        "            chunks = self.compress_documents(chunks)\n",
        "            logger.info(f\"Number of chunks after compression: {len(chunks)}\")\n",
        "\n",
        "\n",
        "        # You could potentially filter/rerank chunks here if you want to reduce the\n",
        "        # number of chunks included in the prompt (e.g., top N based on a test query or simple selection)\n",
        "        # Currently, we'll include all processed chunks in the prompt context.\n",
        "        # If chunks are too many/long for the model's context window, this needs refinement (e.g., select top K relevant chunks per query).\n",
        "        # For simplicity now, we store all processed chunks.\n",
        "\n",
        "        self.knowledge_chunks = chunks\n",
        "        logger.info(f\"Finished processing document. Stored {len(self.knowledge_chunks)} chunks.\")\n",
        "        return True\n",
        "\n",
        "\n",
        "    def contextual_qa_chat(self, query: str, max_new_tokens: int = 150):\n",
        "        \"\"\"\n",
        "        Trả lời câu hỏi của người dùng, kết hợp kiến thức từ tài liệu và lịch sử hội thoại.\n",
        "\n",
        "        Args:\n",
        "            query: Câu hỏi hiện tại của người dùng.\n",
        "            max_new_tokens: Số lượng token mới tối đa cho câu trả lời.\n",
        "\n",
        "        Returns:\n",
        "            str: Câu trả lời của AI.\n",
        "        \"\"\"\n",
        "        if self.llm is None or self.tokenizer is None:\n",
        "             return \"Hệ thống chưa sẵn sàng. Vui lòng kiểm tra quá trình tải mô hình.\"\n",
        "\n",
        "        if not self.knowledge_chunks:\n",
        "             logger.warning(\"No knowledge chunks available. Answering based on general knowledge.\")\n",
        "             # If no document is processed, create a simple prompt without context\n",
        "             system_message = \"Bạn là trợ lý AI.\"\n",
        "             # Format the prompt for a simple chat turn\n",
        "             messages = [\n",
        "                 {\"role\": \"system\", \"content\": system_message},\n",
        "             ]\n",
        "             # Add conversation history if any\n",
        "             if self.conversation_history:\n",
        "                  hist_context = self.generate_conversation_context()\n",
        "                  messages.append({\"role\": \"user\", \"content\": hist_context}) # Add history as a user turn? Or system? Let's simplify\n",
        "                  # A better approach for history without RAG context might be different.\n",
        "                  # For now, let's just add the current query if no chunks and no history.\n",
        "                  # If history exists, include it.\n",
        "                  user_content = self.generate_conversation_context()\n",
        "                  if user_content:\n",
        "                       user_content += \"\\n\"\n",
        "                  user_content += f\"Câu hỏi: {query}\"\n",
        "                  messages.append({\"role\": \"user\", \"content\": user_content})\n",
        "\n",
        "\n",
        "             else: # No history, no chunks\n",
        "                 messages.append({\"role\": \"user\", \"content\": query}) # Just the current query\n",
        "\n",
        "\n",
        "             # Apply chat template for the full prompt\n",
        "             full_prompt = self.tokenizer.apply_chat_template(\n",
        "                 messages,\n",
        "                 tokenize=False,\n",
        "                 add_generation_prompt=True # Adds <|im_start|>assistant\\n\n",
        "             )\n",
        "\n",
        "        else:\n",
        "            # --- 1. Build the full prompt with Knowledge + History + Query ---\n",
        "            system_message, knowledge_context_string = self.preprocess_knowledge_prefix(self.knowledge_chunks)\n",
        "            conversation_context = self.generate_conversation_context()\n",
        "\n",
        "            # Combine all parts into the user's message content\n",
        "            user_content = knowledge_context_string\n",
        "            if conversation_context:\n",
        "                 user_content += \"\\n\\n\" + conversation_context # Add history after knowledge\n",
        "\n",
        "            user_content += f\"\\n\\nCâu hỏi: {query}\"\n",
        "\n",
        "\n",
        "            # Construct the list of messages for the chat template\n",
        "            # Qwen2 template: <|im_start|>system\\n{sys}<|im_end|>\\n<|im_start|>user\\n{user}<|im_end|>\\n<|im_start|>assistant\\n\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": user_content}\n",
        "            ]\n",
        "\n",
        "            # Apply chat template to get the full prompt string for vLLM\n",
        "            full_prompt = self.tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=True # Adds <|im_start|>assistant\\n at the end\n",
        "            )\n",
        "            # logger.debug(f\"Full prompt sent to vLLM:\\n---\\n{full_prompt}\\n---\")\n",
        "\n",
        "            # --- Check prompt length (optional but recommended) ---\n",
        "            # prompt_token_ids = self.tokenizer.encode(full_prompt)\n",
        "            # if len(prompt_token_ids) > self.llm.get_model_config().get_max_model_len():\n",
        "            #     logger.warning(f\"Prompt length ({len(prompt_token_ids)} tokens) exceeds model max length. Response might be cut off or garbled.\")\n",
        "                # Implement truncation logic here if needed (e.g., reduce chunks, reduce history)\n",
        "\n",
        "\n",
        "        # --- 2. Generate the answer using vLLM ---\n",
        "        # Pass the full prompt string to the answer_question method\n",
        "        answer = self.answer_question(full_prompt, max_new_tokens)\n",
        "\n",
        "        # --- 3. Add query and response to conversation history ---\n",
        "        self.add_to_conversation(query, answer)\n",
        "\n",
        "        return answer\n",
        "\n",
        "    def save_conversation_history(self, file_path):\n",
        "        \"\"\"\n",
        "        Lưu toàn bộ lịch sử các phiên cuộc trò chuyện\n",
        "        \"\"\"\n",
        "        # Implementation remains the same as it saves the sessions dictionary\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "        history_data = {}\n",
        "        for session_id, conversations in self.sessions.items():\n",
        "             history_data[session_id] = [\n",
        "                 {\"query\": c[\"query\"], \"response\": c[\"response\"]}\n",
        "                 for c in conversations\n",
        "             ]\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(history_data, f, ensure_ascii=False, indent=2)\n",
        "            logger.info(f\"Conversation history saved to {file_path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "             logger.error(f\"Error saving conversation history to {file_path}: {e}\")\n",
        "             return False\n",
        "\n",
        "\n",
        "    def load_conversation_history(self, file_path):\n",
        "        \"\"\"\n",
        "        Tải toàn bộ lịch sử các phiên cuộc trò chuyện\n",
        "        \"\"\"\n",
        "        # Implementation remains the same as it loads into the sessions dictionary\n",
        "        if not os.path.exists(file_path):\n",
        "             logger.info(f\"Conversation history file not found: {file_path}\")\n",
        "             return False\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                history_data = json.load(f)\n",
        "\n",
        "            self.sessions = history_data\n",
        "            logger.info(f\"Loaded conversation history containing {len(self.sessions)} sessions.\")\n",
        "\n",
        "            # After loading, if there's a last session, set it as the current one\n",
        "            if self.sessions:\n",
        "                # Get the last session ID (assuming dictionary iteration order is insertion order in recent Python)\n",
        "                last_session_id = list(self.sessions.keys())[-1]\n",
        "                self.current_session_id = last_session_id\n",
        "                # Set conversation_history for the current session, limited by memory size\n",
        "                self.conversation_history = self.sessions[self.current_session_id][-self.conversation_memory_size:]\n",
        "                logger.info(f\"Set current session to: {self.current_session_id}\")\n",
        "            else:\n",
        "                self.current_session_id = None\n",
        "                self.conversation_history = []\n",
        "\n",
        "            return True\n",
        "        except Exception as e:\n",
        "             logger.error(f\"Error loading conversation history from {file_path}: {e}\")\n",
        "             # Clear potentially partially loaded data\n",
        "             self.sessions = {}\n",
        "             self.current_session_id = None\n",
        "             self.conversation_history = []\n",
        "             return False\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9HwaFUlHoba",
        "outputId": "58660362-bc8e-41b5-ae7e-41a3776bdfda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cag.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_cag.py\n",
        "from typing import Any, List, Optional\n",
        "# Thêm các import cần thiết\n",
        "from fastapi import FastAPI, HTTPException, UploadFile, File, Form, Request, Response, Depends\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel, Field\n",
        "import torch\n",
        "import os\n",
        "import uuid\n",
        "from contextlib import asynccontextmanager\n",
        "from pymongo import MongoClient\n",
        "from pymongo.errors import ConnectionFailure, OperationFailure\n",
        "import bcrypt\n",
        "# Import class CAG thay vì RAG\n",
        "from cag import ContextualCAG # Đảm bảo file cag.py cùng thư mục hoặc trong PYTHONPATH\n",
        "from bson.objectid import ObjectId\n",
        "import json\n",
        "import logging\n",
        "import time # Thêm time nếu cần đo thời gian xử lý\n",
        "\n",
        "# Cấu hình logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Khởi tạo lifespan của ứng dụng để tải mô hình CAG\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    logger.info(\"Starting application lifespan for CAG...\")\n",
        "    # Initialize CAG model\n",
        "    try:\n",
        "        # Thay ContextualRetrieval bằng ContextualCAG\n",
        "        app.state.cag_model = ContextualCAG()\n",
        "        # Không cần app.state.faiss_db nữa vì CAG quản lý knowledge chunks nội bộ\n",
        "        logger.info(\"CAG model initialized.\")\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Failed to initialize CAG model during startup!\")\n",
        "        raise RuntimeError(\"CAG model initialization failed\") from e\n",
        "\n",
        "    # Load existing conversation history if available (logic này vẫn giữ nguyên)\n",
        "    history_path = \"conversation_history.json\"\n",
        "    if os.path.exists(history_path):\n",
        "        try:\n",
        "            # Gọi phương thức của cag_model\n",
        "            loaded = app.state.cag_model.load_conversation_history(history_path)\n",
        "            if loaded:\n",
        "                logger.info(f\"Loaded conversation history from {history_path}\")\n",
        "            else:\n",
        "                logger.warning(f\"Failed to load conversation history from {history_path}, but file exists.\")\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"Error loading conversation history from {history_path}\")\n",
        "    else:\n",
        "        logger.info(f\"Conversation history file not found at {history_path}, starting fresh.\")\n",
        "\n",
        "    # Ensure upload directory exists (vẫn cần thiết)\n",
        "    upload_dir = \"uploaded_files\"\n",
        "    if not os.path.exists(upload_dir):\n",
        "        try:\n",
        "            os.makedirs(upload_dir)\n",
        "            logger.info(f\"Created directory: {upload_dir}\")\n",
        "        except OSError as e:\n",
        "            logger.error(f\"Failed to create upload directory {upload_dir}: {e}\")\n",
        "\n",
        "    # --- Kết nối MongoDB (vẫn giữ nguyên) ---\n",
        "    MONGO_URI = os.getenv(\"MONGO_URI\", \"mongodb+srv://hoangkahn123:0968269517@cluster0.8to29z8.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\")\n",
        "    app.state.mongo_client = None\n",
        "    app.state.users_collection = None\n",
        "    try:\n",
        "        logger.info(f\"Connecting to MongoDB at {MONGO_URI.split('@')[-1].split('/')[0]}...\")\n",
        "        app.state.mongo_client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)\n",
        "        app.state.mongo_client.admin.command('ping')\n",
        "        db = app.state.mongo_client[\"userdb\"]\n",
        "        app.state.users_collection = db[\"users\"]\n",
        "        logger.info(\"Successfully connected to MongoDB.\")\n",
        "    except ConnectionFailure as e:\n",
        "        logger.error(f\"Failed to connect to MongoDB: {e}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An unexpected error occurred during MongoDB connection: {e}\")\n",
        "    # -----------------------\n",
        "\n",
        "    yield # Application is ready to serve requests\n",
        "\n",
        "    # --- Shutdown ---\n",
        "    logger.info(\"Shutting down application lifespan (CAG)...\")\n",
        "    # Save conversation history before shutdown\n",
        "    if hasattr(app.state, 'cag_model'):\n",
        "        try:\n",
        "            app.state.cag_model.save_conversation_history(\"conversation_history.json\")\n",
        "            logger.info(\"Saved conversation history.\")\n",
        "        except Exception as e:\n",
        "            logger.exception(\"Error saving conversation history during shutdown\")\n",
        "\n",
        "    # Close MongoDB connection\n",
        "    if app.state.mongo_client:\n",
        "        try:\n",
        "            app.state.mongo_client.close()\n",
        "            logger.info(\"MongoDB connection closed.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error closing MongoDB connection: {e}\")\n",
        "    # ----------------\n",
        "\n",
        "app = FastAPI(lifespan=lifespan)\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"http://localhost:5173\"], # Cập nhật cho production\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\", \"Authorization\"],\n",
        ")\n",
        "\n",
        "# Các model Pydantic (Giữ nguyên cấu trúc, có thể sửa message nếu cần)\n",
        "class UserCredentials(BaseModel):\n",
        "    username: str\n",
        "    password: str\n",
        "\n",
        "class LoginResponse(BaseModel):\n",
        "    username: str\n",
        "    id: str\n",
        "    session_id: str\n",
        "\n",
        "class RegisterResponse(BaseModel):\n",
        "    message: str\n",
        "    session_id: str\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    query: str\n",
        "    # Optional: Thêm max_new_tokens nếu muốn client điều khiển\n",
        "    # max_new_tokens: Optional[int] = 300\n",
        "\n",
        "class ChatResponse(BaseModel):\n",
        "    response: str\n",
        "    session_id: str\n",
        "\n",
        "class SessionResponse(BaseModel):\n",
        "    session_id: str\n",
        "    message: str\n",
        "\n",
        "class DocumentUploadResponse(BaseModel):\n",
        "    message: str\n",
        "    # Document ID có thể vẫn hữu ích để tham chiếu file đã upload\n",
        "    document_id: str\n",
        "\n",
        "class StatusResponse(BaseModel):\n",
        "    status: str\n",
        "    gpu: str\n",
        "\n",
        "class SessionListResponse(BaseModel):\n",
        "    sessions: List[str]\n",
        "\n",
        "class HistoryItem(BaseModel):\n",
        "    query: Optional[str] = None\n",
        "    response: Optional[str] = None\n",
        "    # Lưu ý: CAG class không lưu timestamp trong history, nên field này sẽ là None\n",
        "    timestamp: Optional[str] = None\n",
        "\n",
        "class HistoryResponse(BaseModel):\n",
        "    session_id: str\n",
        "    history: List[HistoryItem]\n",
        "\n",
        "class LogoutResponse(BaseModel):\n",
        "    message: str\n",
        "\n",
        "# Hàm phụ thuộc để lấy DB collection (Giữ nguyên)\n",
        "def get_users_collection():\n",
        "    if not hasattr(app.state, 'users_collection') or app.state.users_collection is None:\n",
        "        logger.error(\"Users collection is not available.\")\n",
        "        raise HTTPException(status_code=503, detail=\"Database service unavailable\")\n",
        "    return app.state.users_collection\n",
        "\n",
        "# Hàm phụ thuộc để lấy CAG model\n",
        "def get_cag_model() -> ContextualCAG:\n",
        "    if not hasattr(app.state, 'cag_model') or app.state.cag_model is None:\n",
        "        logger.error(\"CAG model is not available.\")\n",
        "        raise HTTPException(status_code=503, detail=\"CAG service unavailable\")\n",
        "    return app.state.cag_model\n",
        "\n",
        "# Hàm phụ thuộc để lấy session từ token (Sửa để dùng get_cag_model)\n",
        "async def get_session_from_token(request: Request):\n",
        "    \"\"\"Extract and validate session token from Authorization header.\"\"\"\n",
        "    auth_header = request.headers.get(\"Authorization\")\n",
        "    if not auth_header or not auth_header.startswith(\"Bearer \"):\n",
        "        logger.warning(\"Access attempt failed: No valid Authorization header found.\")\n",
        "        raise HTTPException(status_code=401, detail=\"Unauthorized: Invalid or missing token\")\n",
        "\n",
        "    session_id = auth_header.split(\" \")[1]\n",
        "\n",
        "    # Sử dụng cag_model để kiểm tra session\n",
        "    cag_model = get_cag_model()\n",
        "    if not cag_model.load_session(session_id):\n",
        "        logger.warning(f\"Access attempt failed: Session ID '{session_id}' not found in CAG model memory.\")\n",
        "        # Quan trọng: load_session của CAG chỉ trả về True/False, không raise lỗi\n",
        "        # Nên chúng ta cần raise lỗi ở đây nếu không load được\n",
        "        raise HTTPException(status_code=401, detail=\"Invalid or expired session. Please login again.\")\n",
        "\n",
        "    # Nếu load thành công, trả về session_id để endpoint sử dụng\n",
        "    return session_id\n",
        "\n",
        "# --- Endpoints API ---\n",
        "\n",
        "@app.get(\"/\", response_model=StatusResponse)\n",
        "def status_gpu_check():\n",
        "    \"\"\"Kiểm tra trạng thái API (CAG) và GPU.\"\"\"\n",
        "    cag_model = get_cag_model() # Check if CAG model loaded\n",
        "    gpu_available = torch.cuda.is_available()\n",
        "    gpu_msg = f\"Available ({torch.cuda.get_device_name(0)})\" if gpu_available else \"Unavailable\"\n",
        "    return {\n",
        "        # Cập nhật status message\n",
        "        \"status\": \"CAG API is running\",\n",
        "        \"gpu\": gpu_msg\n",
        "    }\n",
        "\n",
        "@app.post(\"/upload/pdf\", response_model=DocumentUploadResponse)\n",
        "async def upload_pdf(file: UploadFile = File(...)):\n",
        "    \"\"\"Upload và xử lý file PDF để lưu trữ knowledge chunks.\"\"\"\n",
        "    # Lấy CAG model\n",
        "    cag_model = get_cag_model()\n",
        "    if not file.filename.endswith('.pdf'):\n",
        "        logger.warning(f\"Upload rejected: Non-PDF file '{file.filename}'\")\n",
        "        raise HTTPException(status_code=400, detail=\"Only PDF files are accepted\")\n",
        "\n",
        "    # Tạo ID để quản lý file upload, dù CAG không dùng trực tiếp để chọn context\n",
        "    document_id = str(uuid.uuid4())\n",
        "    file_path = f\"uploaded_files/{document_id}.pdf\"\n",
        "    logger.info(f\"Received PDF upload: '{file.filename}', saving as {document_id}.pdf\")\n",
        "\n",
        "    try:\n",
        "        # Lưu file\n",
        "        file_content = await file.read()\n",
        "        if not file_content:\n",
        "            raise HTTPException(status_code=400, detail=\"Uploaded file is empty\")\n",
        "        with open(file_path, \"wb\") as buffer:\n",
        "            buffer.write(file_content)\n",
        "        logger.info(f\"Saved uploaded file to: {file_path}\")\n",
        "\n",
        "        # Xử lý tài liệu bằng CAG model (lưu chunks vào self.knowledge_chunks)\n",
        "        logger.info(f\"Processing PDF document with CAG: {document_id}.pdf\")\n",
        "        # process_pdf_document trả về True/False\n",
        "        processed_ok = cag_model.process_pdf_document(file_path) # Giả sử use_compression=True mặc định\n",
        "\n",
        "        if not processed_ok:\n",
        "             logger.error(f\"CAG model failed to process document {document_id}.pdf (check logs for details).\")\n",
        "             # Có thể do file trống sau khi load, hoặc lỗi khác trong CAG class\n",
        "             raise HTTPException(status_code=500, detail=\"Failed to process PDF content.\")\n",
        "\n",
        "        logger.info(f\"PDF processed successfully by CAG. Knowledge chunks updated for document {document_id}.\")\n",
        "\n",
        "        return {\n",
        "            \"message\": \"PDF uploaded and processed successfully by CAG\",\n",
        "            \"document_id\": document_id\n",
        "        }\n",
        "    except FileNotFoundError as fnf_err:\n",
        "        logger.error(f\"Error during PDF processing: {fnf_err}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Server error: Could not find file during processing.\")\n",
        "    except ValueError as val_err: # Bắt lỗi từ CAG class (vd: empty docs)\n",
        "        logger.error(f\"Value error during PDF processing: {val_err}\")\n",
        "        raise HTTPException(status_code=400, detail=str(val_err))\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Unexpected error processing PDF {document_id}.pdf with CAG\")\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                os.remove(file_path)\n",
        "                logger.info(f\"Removed temporary file due to error: {file_path}\")\n",
        "            except OSError as rm_err:\n",
        "                logger.error(f\"Error removing file {file_path} after error: {rm_err}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Error processing PDF with CAG: {str(e)}\")\n",
        "    finally:\n",
        "        await file.close()\n",
        "\n",
        "@app.post(\"/chat\", response_model=ChatResponse)\n",
        "async def chat(request_body: ChatRequest, session_id: str = Depends(get_session_from_token)):\n",
        "    \"\"\"Xử lý yêu cầu chat, sử dụng session và knowledge chunks từ CAG model.\"\"\"\n",
        "    cag_model = get_cag_model()\n",
        "\n",
        "    logger.info(f\"Received chat request for session: {session_id} | Query: '{request_body.query[:50]}...'\")\n",
        "\n",
        "    # Kiểm tra xem CAG model đã xử lý tài liệu nào chưa\n",
        "    if not cag_model.knowledge_chunks:\n",
        "        logger.warning(f\"Chat attempt failed for session {session_id}: No knowledge chunks loaded in CAG model.\")\n",
        "        # Trả lời chung hoặc thông báo lỗi tùy yêu cầu\n",
        "        # Có thể gọi contextual_qa_chat, nó sẽ tự xử lý trường hợp không có chunks\n",
        "        # raise HTTPException(status_code=400, detail=\"No document has been uploaded and processed yet. Please upload a PDF first.\")\n",
        "        logger.info(f\"No knowledge chunks. CAG will answer based on general knowledge / history for session {session_id}.\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Session đã được load và xác thực bởi get_session_from_token\n",
        "\n",
        "        # Xử lý chat request bằng CAG model\n",
        "        logger.info(f\"Processing query with CAG for session {session_id}...\")\n",
        "        # Lấy max_new_tokens từ request nếu có, hoặc dùng default của CAG class\n",
        "        # max_tokens = request_body.max_new_tokens if hasattr(request_body, 'max_new_tokens') else 300 # Ví dụ\n",
        "        response_text = cag_model.contextual_qa_chat(request_body.query) # Giả sử dùng max_tokens mặc định\n",
        "\n",
        "        logger.info(f\"Generated CAG response for session {session_id}\")\n",
        "\n",
        "        return {\n",
        "            \"response\": response_text,\n",
        "            \"session_id\": session_id\n",
        "        }\n",
        "    except HTTPException as http_exc:\n",
        "        raise http_exc\n",
        "    except ValueError as ve: # Lỗi từ CAG logic\n",
        "        logger.error(f\"Value error during CAG chat for session {session_id}: {ve}\")\n",
        "        raise HTTPException(status_code=500, detail=str(ve))\n",
        "    except RuntimeError as re: # Lỗi runtime từ CAG/vLLM\n",
        "        logger.error(f\"Runtime error during CAG chat for session {session_id}: {re}\")\n",
        "        raise HTTPException(status_code=500, detail=str(re))\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Unexpected error during CAG chat processing for session {session_id}\")\n",
        "        raise HTTPException(status_code=500, detail=\"Internal server error during CAG chat processing\")\n",
        "\n",
        "\n",
        "# Các endpoint quản lý session và history: chỉ cần thay get_rag_model -> get_cag_model\n",
        "\n",
        "@app.get(\"/sessions\", response_model=SessionListResponse)\n",
        "def list_sessions(session_id: str = Depends(get_session_from_token)):\n",
        "    \"\"\"Liệt kê tất cả các session IDs đang có trong CAG model memory.\"\"\"\n",
        "    cag_model = get_cag_model()\n",
        "    sessions = list(cag_model.sessions.keys())\n",
        "    return {\"sessions\": sessions}\n",
        "\n",
        "@app.get(\"/history\", response_model=HistoryResponse)\n",
        "def get_current_session_history(session_id: str = Depends(get_session_from_token)):\n",
        "    \"\"\"Lấy lịch sử trò chuyện của session hiện tại từ CAG model memory.\"\"\"\n",
        "    cag_model = get_cag_model()\n",
        "\n",
        "    history = []\n",
        "    if session_id in cag_model.sessions:\n",
        "        for conv in cag_model.sessions[session_id]:\n",
        "             # CAG history chỉ có query/response\n",
        "            history.append(HistoryItem(\n",
        "                query=conv.get(\"query\"),\n",
        "                response=conv.get(\"response\"),\n",
        "                timestamp=None # CAG class không lưu timestamp\n",
        "            ))\n",
        "    else:\n",
        "        # Trường hợp session_id hợp lệ (từ token) nhưng không có trong sessions? (khó xảy ra nếu get_session_from_token hoạt động đúng)\n",
        "        logger.warning(f\"Session {session_id} validated but not found in cag_model.sessions for history request.\")\n",
        "\n",
        "    return {\"session_id\": session_id, \"history\": history}\n",
        "\n",
        "@app.get(\"/history/{target_session_id}\", response_model=HistoryResponse)\n",
        "def get_specific_session_history(\n",
        "    target_session_id: str,\n",
        "    current_session_id: str = Depends(get_session_from_token) # Vẫn cần xác thực người dùng hiện tại\n",
        "):\n",
        "    \"\"\"Lấy lịch sử trò chuyện của một session cụ thể từ CAG model memory.\"\"\"\n",
        "    cag_model = get_cag_model()\n",
        "    if target_session_id not in cag_model.sessions:\n",
        "        raise HTTPException(status_code=404, detail=\"Target session not found in CAG model memory\")\n",
        "\n",
        "    history = []\n",
        "    for conv in cag_model.sessions[target_session_id]:\n",
        "        history.append(HistoryItem(\n",
        "            query=conv.get(\"query\"),\n",
        "            response=conv.get(\"response\"),\n",
        "            timestamp=None # CAG class không lưu timestamp\n",
        "        ))\n",
        "\n",
        "    return {\"session_id\": target_session_id, \"history\": history}\n",
        "\n",
        "@app.delete(\"/sessions\", response_model=SessionResponse)\n",
        "def delete_all_sessions(session_id: str = Depends(get_session_from_token)):\n",
        "    \"\"\"Xóa tất cả các session khỏi CAG model memory (Cẩn thận khi dùng).\"\"\"\n",
        "    cag_model = get_cag_model()\n",
        "    count = len(cag_model.sessions)\n",
        "    cag_model.sessions.clear()\n",
        "    cag_model.current_session_id = None # Reset cả session đang active\n",
        "    cag_model.conversation_history = [] # Clear history đang active\n",
        "    logger.warning(f\"Cleared all {count} sessions from CAG model memory.\")\n",
        "    return {\n",
        "        \"session_id\": \"all\",\n",
        "        \"message\": f\"All {count} sessions deleted successfully from memory\"\n",
        "    }\n",
        "\n",
        "# --- Endpoints Đăng ký / Đăng nhập / Đăng xuất (Chỉ cần thay get_rag_model -> get_cag_model) ---\n",
        "\n",
        "@app.post(\"/register/\", response_model=RegisterResponse, status_code=201)\n",
        "async def register_user(user: UserCredentials):\n",
        "    \"\"\"Đăng ký người dùng mới.\"\"\"\n",
        "    users_collection = get_users_collection()\n",
        "    # Lấy CAG model\n",
        "    cag_model = get_cag_model()\n",
        "\n",
        "    existing_user = users_collection.find_one({\"username\": {\"$regex\": f\"^{user.username}$\", \"$options\": \"i\"}})\n",
        "    if existing_user:\n",
        "        logger.warning(f\"Registration failed: Username '{user.username}' already exists.\")\n",
        "        raise HTTPException(status_code=400, detail=\"Username already exists\")\n",
        "\n",
        "    try:\n",
        "        hashed_password = bcrypt.hashpw(user.password.encode(\"utf-8\"), bcrypt.gensalt())\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error hashing password for {user.username}: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=\"Error processing password\")\n",
        "\n",
        "    # Tạo session CAG mới cho user\n",
        "    session_id = cag_model.start_new_session()\n",
        "    logger.info(f\"Started new CAG session {session_id} for new user {user.username}\")\n",
        "\n",
        "    try:\n",
        "        user_data = {\n",
        "            \"username\": user.username,\n",
        "            \"password\": hashed_password,\n",
        "            \"session_id\": session_id # Lưu session_id liên kết với user\n",
        "        }\n",
        "        insert_result = users_collection.insert_one(user_data)\n",
        "        logger.info(f\"Registered user {user.username} with DB ID {insert_result.inserted_id} and CAG session {session_id}\")\n",
        "        return RegisterResponse(message=\"User registered successfully\", session_id=session_id)\n",
        "    except OperationFailure as e:\n",
        "        logger.error(f\"Database error during registration for {user.username}: {e.details}\")\n",
        "        # Rollback: Xóa session vừa tạo trong cag_model\n",
        "        if session_id in cag_model.sessions:\n",
        "             del cag_model.sessions[session_id]\n",
        "             if cag_model.current_session_id == session_id:\n",
        "                 cag_model.current_session_id = None\n",
        "                 cag_model.conversation_history = []\n",
        "             logger.info(f\"Rolled back CAG session {session_id} due to DB error.\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Database error during registration\")\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Unexpected error registering user {user.username}\")\n",
        "        raise HTTPException(status_code=500, detail=\"Internal server error during registration\")\n",
        "\n",
        "@app.post(\"/login/\", response_model=LoginResponse)\n",
        "async def login_user(user: UserCredentials):\n",
        "    \"\"\"Đăng nhập người dùng và trả về token (session_id của CAG).\"\"\"\n",
        "    users_collection = get_users_collection()\n",
        "    # Lấy CAG model\n",
        "    cag_model = get_cag_model()\n",
        "    logger.info(f\"Login attempt for user: {user.username}\")\n",
        "\n",
        "    found_user = users_collection.find_one({\"username\": {\"$regex\": f\"^{user.username}$\", \"$options\": \"i\"}})\n",
        "\n",
        "    if found_user and bcrypt.checkpw(user.password.encode(\"utf-8\"), found_user[\"password\"]):\n",
        "        user_id_str = str(found_user[\"_id\"])\n",
        "        session_id = found_user.get(\"session_id\")\n",
        "        session_loaded_or_created = False\n",
        "\n",
        "        if session_id and cag_model.load_session(session_id):\n",
        "            logger.info(f\"Loaded existing CAG session {session_id} for user {user.username}\")\n",
        "            session_loaded_or_created = True\n",
        "        else:\n",
        "            if session_id:\n",
        "                logger.warning(f\"Session ID {session_id} for user {user.username} found in DB but not in CAG model memory. Starting new session.\")\n",
        "            else:\n",
        "                logger.warning(f\"No session ID found in DB for user {user.username}. Starting new session.\")\n",
        "\n",
        "            # Tạo session CAG mới và cập nhật DB\n",
        "            session_id = cag_model.start_new_session()\n",
        "            try:\n",
        "                update_result = users_collection.update_one(\n",
        "                    {\"_id\": found_user[\"_id\"]},\n",
        "                    {\"$set\": {\"session_id\": session_id}}\n",
        "                )\n",
        "                if update_result.modified_count == 1:\n",
        "                    logger.info(f\"Updated user {user.username} in DB with new CAG session ID {session_id}\")\n",
        "                    session_loaded_or_created = True\n",
        "                else:\n",
        "                    logger.error(f\"Failed to update session ID in DB for user {user.username}\")\n",
        "                    # Rollback session CAG đã tạo?\n",
        "                    if session_id in cag_model.sessions: del cag_model.sessions[session_id]\n",
        "                    raise HTTPException(status_code=500, detail=\"Failed to update session in database\")\n",
        "\n",
        "            except OperationFailure as e:\n",
        "                logger.error(f\"Database error updating session ID for user {user.username}: {e.details}\")\n",
        "                if session_id in cag_model.sessions: del cag_model.sessions[session_id]\n",
        "                raise HTTPException(status_code=500, detail=\"Database error during session update\")\n",
        "            except Exception as e:\n",
        "                logger.exception(f\"Unexpected error updating session ID for {user.username}\")\n",
        "                if session_id in cag_model.sessions: del cag_model.sessions[session_id]\n",
        "                raise HTTPException(status_code=500, detail=\"Internal server error during session update\")\n",
        "\n",
        "        if session_loaded_or_created:\n",
        "            logger.info(f\"Login successful for {user.username}. Returning CAG session token {session_id}\")\n",
        "            return LoginResponse(\n",
        "                username=found_user[\"username\"],\n",
        "                id=user_id_str,\n",
        "                session_id=session_id\n",
        "            )\n",
        "        else:\n",
        "            logger.error(f\"Login failed for {user.username} despite credentials being correct - CAG session could not be loaded or created.\")\n",
        "            raise HTTPException(status_code=500, detail=\"Internal server error during session handling\")\n",
        "\n",
        "    else:\n",
        "        logger.warning(f\"Login failed for user {user.username}: Invalid username or password.\")\n",
        "        raise HTTPException(status_code=401, detail=\"Invalid username or password\")\n",
        "\n",
        "\n",
        "@app.post(\"/logout/{user_id}\", response_model=LogoutResponse)\n",
        "async def logout_user(user_id: str, session_id: str = Depends(get_session_from_token)):\n",
        "    \"\"\"Đăng xuất người dùng (có thể thêm logic xóa session nếu cần).\"\"\"\n",
        "    users_collection = get_users_collection()\n",
        "    # Lấy CAG model\n",
        "    cag_model = get_cag_model()\n",
        "    logger.info(f\"Logout request for user ID: {user_id} with active session {session_id}\")\n",
        "\n",
        "    try:\n",
        "        if not ObjectId.is_valid(user_id):\n",
        "            logger.warning(f\"Logout failed: Invalid user ID format '{user_id}'\")\n",
        "            raise HTTPException(status_code=400, detail=\"Invalid user ID format\")\n",
        "\n",
        "        found_user = users_collection.find_one({\"_id\": ObjectId(user_id)})\n",
        "        username = \"N/A\"\n",
        "        if found_user:\n",
        "            username = found_user.get(\"username\", \"N/A\")\n",
        "            stored_session_id = found_user.get(\"session_id\")\n",
        "            if stored_session_id != session_id:\n",
        "                 logger.warning(f\"User {username} (ID: {user_id}) logging out with token session {session_id}, but DB stores session {stored_session_id}. Proceeding with logout based on token.\")\n",
        "\n",
        "        # Logic logout tùy chọn: Xóa session khỏi memory? Xóa session_id khỏi DB?\n",
        "        # Ví dụ: Xóa session khỏi memory nếu nó tồn tại\n",
        "        if session_id in cag_model.sessions:\n",
        "            del cag_model.sessions[session_id]\n",
        "            logger.info(f\"Removed session {session_id} from CAG model memory during logout for user {username}.\")\n",
        "            # Nếu session bị xóa là session hiện tại của model thì reset\n",
        "            if cag_model.current_session_id == session_id:\n",
        "                 cag_model.current_session_id = None\n",
        "                 cag_model.conversation_history = []\n",
        "\n",
        "        # Ví dụ: Xóa session_id khỏi DB (cẩn thận nếu user có thể login từ nhiều nơi)\n",
        "        # users_collection.update_one({\"_id\": ObjectId(user_id)}, {\"$unset\": {\"session_id\": \"\"}})\n",
        "        # logger.info(f\"Removed session ID reference from DB for user {username}.\")\n",
        "\n",
        "        logger.info(f\"User {username} (ID: {user_id}) logout processed successfully.\")\n",
        "        return LogoutResponse(message=\"Successfully logged out\")\n",
        "    except HTTPException as http_exc:\n",
        "        raise http_exc\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Error during logout processing for user ID {user_id}\")\n",
        "        return LogoutResponse(message=\"Logout processed with server error\")\n",
        "\n",
        "\n",
        "# Chạy app nếu file được thực thi trực tiếp\n",
        "# if __name__ == \"__main__\":\n",
        "#     import uvicorn\n",
        "#     # Port mặc định 8000, có thể thay đổi\n",
        "#     # Host 0.0.0.0 để có thể truy cập từ bên ngoài container/máy ảo\n",
        "#     uvicorn.run(\"app_cag:app\", host=\"0.0.0.0\", port=8000, reload=True) # reload=True chỉ dùng cho dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IjFnzbDQuf6",
        "outputId": "cb687351-4b04-4419-c197-eab7236250da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_cag.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SxqDpmlTNgM",
        "outputId": "8aeefa58-ce1e-4fea-82ce-cfc0a5b21022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf2\n",
            "Successfully installed pypdf2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "from ipywidgets import HTML\n",
        "from IPython.display import display\n",
        "\n",
        "t = HTML(\n",
        "    value=\"0 Seconds\",\n",
        "    description='Server is Starting Up... Elapsed Time:',\n",
        "    style={'description_width': 'initial'},\n",
        ")\n",
        "display(t)\n",
        "\n",
        "flag = True\n",
        "timer = 0\n",
        "\n",
        "try:\n",
        "    subprocess.check_output(['curl', \"localhost:8000\"])\n",
        "    flag = False\n",
        "except:\n",
        "    # Khởi động server với chế độ reload\n",
        "    get_ipython().system_raw('uvicorn app_cag:app --host 0.0.0.0 --port 8000 --reload > server.log 2>&1 &')\n",
        "\n",
        "while flag and timer < 600:\n",
        "    try:\n",
        "        subprocess.check_output(['curl', \"localhost:8000\"])\n",
        "    except:\n",
        "        time.sleep(1)\n",
        "        timer += 1\n",
        "        t.value = str(timer) + \" Seconds\"\n",
        "    else:\n",
        "        flag = False\n",
        "\n",
        "if timer >= 600:\n",
        "    print(\"Error: timed out! Took more than 10 minutes :(\")\n",
        "\n",
        "subprocess.check_output(['curl', \"localhost:8000\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "c2d4ed5a89024130a021ae5243e1f719",
            "c976e072db46490cb7e081659745ef22",
            "e57188ef0ec343a9b0efc38996503ac0"
          ]
        },
        "id": "bYJrD6gzRACC",
        "outputId": "76d549c4-06ef-412e-9b5d-d8b90b175192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='0 Seconds', description='Server is Starting Up... Elapsed Time:', style=DescriptionStyle(descripti…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2d4ed5a89024130a021ae5243e1f719"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'{\"status\":\"CAG API is running\",\"gpu\":\"Available (Tesla T4)\"}'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This starts Ngrok and creates the public URL\n",
        "import subprocess\n",
        "import time\n",
        "import sys\n",
        "import json\n",
        "\n",
        "from IPython import get_ipython\n",
        "get_ipython().system_raw('ngrok http 8000 &')\n",
        "time.sleep(1)\n",
        "curlOut = subprocess.check_output(['curl',\"http://localhost:4040/api/tunnels\"],universal_newlines=True)\n",
        "time.sleep(1)\n",
        "ngrokURL = json.loads(curlOut)['tunnels'][0]['public_url']\n",
        "%store ngrokURL\n",
        "print(ngrokURL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QqasCrbRGqz",
        "outputId": "165cd1ec-4f01-43ae-dda2-c1ac05694d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stored 'ngrokURL' (str)\n",
            "https://b47c-35-185-230-139.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "# Định nghĩa URL của endpoint FastAPI\n",
        "%store -r ngrokURL\n",
        "\n",
        "# Bước 1: Upload tài liệu PDF\n",
        "pdf_path = \"/content/drive/MyDrive/LSVN_TomTat.pdf\"  # Thay đường dẫn tới file PDF của bạn\n",
        "with open(pdf_path, \"rb\") as f:\n",
        "    files = {\"file\": f}\n",
        "    upload_response = requests.post(ngrokURL + \"/upload/pdf\", files=files)\n",
        "\n",
        "if upload_response.status_code == 200:\n",
        "    upload_result = upload_response.json()\n",
        "    print(\"Upload thành công! Document ID:\", upload_result[\"document_id\"])\n",
        "else:\n",
        "    print(\"Upload thất bại với mã trạng thái:\", upload_response.status_code)\n",
        "\n",
        "    exit()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4cUwgcyRJ65",
        "outputId": "0eb7cf9b-f0d8-4689-b33f-424679450b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload thành công! Document ID: 3c8d5a49-34de-4771-a8a4-dd9b6a5f86e8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i :8000"
      ],
      "metadata": {
        "id": "5fwh9DQJ7n3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c2748e6-66b9-40a6-fe78-a62a05f897af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n",
            "uvicorn 3858 root    3u  IPv4  75764      0t0  TCP *:8000 (LISTEN)\n",
            "ngrok   6643 root    6u  IPv4 210192      0t0  TCP localhost:52668->localhost:8000 (ESTABLISHED)\n",
            "ngrok   6643 root    8u  IPv4 213658      0t0  TCP localhost:48188->localhost:8000 (ESTABLISHED)\n",
            "python3 8460 root    3u  IPv4  75764      0t0  TCP *:8000 (LISTEN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 6643 3858 8460"
      ],
      "metadata": {
        "id": "J0ePFX12CXtp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}